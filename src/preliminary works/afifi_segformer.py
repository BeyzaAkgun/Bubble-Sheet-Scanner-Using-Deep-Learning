# -*- coding: utf-8 -*-
"""AfifiSegFormer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iyc8Df3TpFwPbD-cZJiy0G1P4MmiCQ0_
"""

# 1. Gerekli kütüphanelerin yüklenmesi
!pip install transformers==4.35.0 torch torchvision --quiet
!pip install timm datasets --quiet
!pip install --upgrade huggingface_hub transformers --quiet

# 2. İmportlar ve GPU kontrolü
import os
import random
from glob import glob

import numpy as np
import pandas as pd
import cv2
from PIL import Image
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms.functional as TF
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Cihaz kontrolü
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Eğer Google Colab ve Drive kullanıyorsanız:
from google.colab import drive
drive.mount('/content/drive')

# 3. Yol tanımları (ihtiyacınıza göre düzenleyin)
csv_path = '/content/drive/MyDrive/BubbleSheetScannerProject/labeled_data.csv'
image_dir = '/content/drive/MyDrive/BubbleSheetScannerProject/Allimages/'
mask_dir = '/content/drive/MyDrive/BubbleSheetScannerProject/masks/'

# 4. CSV okuma ve var olan eşleşmeleri filtreleme
data = pd.read_csv(csv_path)
# CSV’de dosya adı sütunu 'filename' ise:
filenames = data['filename'].tolist()
image_paths = [os.path.join(image_dir, fn) for fn in filenames]
mask_paths = [os.path.join(mask_dir, fn) for fn in filenames]

paired = [(img, msk) for img, msk in zip(image_paths, mask_paths) if os.path.isfile(img) and os.path.isfile(msk)]
if len(paired) == 0:
    raise RuntimeError("Hiç geçerli image-mask çifti bulunamadı; yol ve dosya adlarını kontrol edin.")
image_paths, mask_paths = zip(*paired)
image_paths = list(image_paths)
mask_paths = list(mask_paths)
print(f"Total valid image-mask pairs: {len(image_paths)}")

# 5. Train/Val/Test split
train_imgs, valtest_imgs, train_masks, valtest_masks = train_test_split(
    image_paths, mask_paths, test_size=0.2, random_state=42
)
val_imgs, test_imgs, val_masks, test_masks = train_test_split(
    valtest_imgs, valtest_masks, test_size=0.5, random_state=42
)
print(f"Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}")

# 6. Dataset sınıfı (augmentation KALDIRILDI, sadece resize & normalize feature_extractor ile yapılacak)
class BubbleSegmentationDataset(Dataset):
    def __init__(self, image_paths, mask_paths, feature_extractor):
        """
        image_paths: list of image file paths
        mask_paths: list of corresponding mask file paths (binary masks with 0/255)
        feature_extractor: SegformerFeatureExtractor instance (resize+normalize için)
        """
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        self.feature_extractor = feature_extractor

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Görüntü ve mask oku
        img = Image.open(self.image_paths[idx]).convert("RGB")
        mask = Image.open(self.mask_paths[idx]).convert("L")  # grayscale

        # Maskı numpy array 0/1 formatına çevir
        mask_np = np.array(mask)
        mask_np = (mask_np > 127).astype(np.uint8)

        # feature_extractor ile resize & normalize:
        # Bu, görüntüyü 256×256 yapacak şekilde ayarlanmalı.
        encoding = self.feature_extractor(images=img,
                                          semantic_masks=mask_np,
                                          return_tensors="pt")
        # encoding: pixel_values: (1,3,H,W), labels: (1,H,W)
        pixel_values = encoding['pixel_values'].squeeze(0)  # (3, H, W)
        labels = encoding['labels'].squeeze(0).long()       # (H, W), 0 veya 1

        return pixel_values, labels

# 7. Feature extractor ve model initialization (256×256 olacak şekilde)
model_name_or_path = "nvidia/segformer-b0-finetuned-ade-512-512"
# Burada size belirtelim:
feature_extractor = SegformerFeatureExtractor.from_pretrained(
    model_name_or_path,
    size={"height": 256, "width": 256}
)

# İki sınıflı segmentasyon için id2label/label2id
id2label = {0: "background", 1: "bubble"}
label2id = {"background": 0, "bubble": 1}

model = SegformerForSemanticSegmentation.from_pretrained(
    model_name_or_path,
    num_labels=2,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True  # classifier head yeniden boyutlanacak
)
model.to(device)

# 8. DataLoader oluşturma
batch_size = 4  # GPU belleğine göre ayarlayın

train_dataset = BubbleSegmentationDataset(train_imgs, train_masks, feature_extractor)
val_dataset = BubbleSegmentationDataset(val_imgs, val_masks, feature_extractor)
test_dataset = BubbleSegmentationDataset(test_imgs, test_masks, feature_extractor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

# 9. Kayıp, optimizer, scheduler
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# 10. Metrik fonksiyonu
def compute_metrics(preds, labels, num_classes=2):
    """
    preds: torch.Tensor shape (N,H,W) değerler 0..num_classes-1
    labels: torch.Tensor shape (N,H,W) değerler 0..num_classes-1
    Pozitif sınıf (bubble=1) için IoU, Dice, precision, recall döner.
    """
    preds = preds.view(-1)
    labels = labels.view(-1)
    tp = ((preds == 1) & (labels == 1)).sum().item()
    fp = ((preds == 1) & (labels == 0)).sum().item()
    fn = ((preds == 0) & (labels == 1)).sum().item()
    epsilon = 1e-6
    iou = tp / (tp + fp + fn + epsilon)
    dice = 2 * tp / (2 * tp + fp + fn + epsilon)
    precision = tp / (tp + fp + epsilon)
    recall = tp / (tp + fn + epsilon)
    return {"iou": iou, "dice": dice, "precision": precision, "recall": recall}

# 11. Eğitim & validation döngüsü
num_epochs = 20
best_val_iou = 0.0
history = {"train_loss": [], "val_loss": [], "val_iou": [], "val_dice": [], "val_precision": [], "val_recall": []}

for epoch in range(num_epochs):
    # Eğitim
    model.train()
    running_loss = 0.0
    loop = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Training")
    for pixel_values, labels in loop:
        pixel_values = pixel_values.to(device)  # (B,3,256,256)
        labels = labels.to(device)              # (B,256,256)

        optimizer.zero_grad()
        outputs = model(pixel_values=pixel_values)
        logits = outputs.logits  # (B,2,256,256)

        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * pixel_values.size(0)
        loop.set_postfix(loss=loss.item())

    epoch_train_loss = running_loss / len(train_dataset)
    history["train_loss"].append(epoch_train_loss)

    # Validation
    model.eval()
    val_running_loss = 0.0
    total_tp = total_fp = total_fn = 0
    with torch.no_grad():
        for pixel_values, labels in tqdm(val_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Validation"):
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            outputs = model(pixel_values=pixel_values)
            logits = outputs.logits
            loss = criterion(logits, labels)
            val_running_loss += loss.item() * pixel_values.size(0)

            preds = torch.argmax(logits, dim=1)  # (B,256,256)
            # Batch TP,FP,FN
            preds_flat = preds.view(-1)
            labels_flat = labels.view(-1)
            tp = ((preds_flat == 1) & (labels_flat == 1)).sum().item()
            fp = ((preds_flat == 1) & (labels_flat == 0)).sum().item()
            fn = ((preds_flat == 0) & (labels_flat == 1)).sum().item()
            total_tp += tp
            total_fp += fp
            total_fn += fn

    epoch_val_loss = val_running_loss / len(val_dataset)
    eps = 1e-6
    val_iou = total_tp / (total_tp + total_fp + total_fn + eps)
    val_dice = 2 * total_tp / (2 * total_tp + total_fp + total_fn + eps)
    val_precision = total_tp / (total_tp + total_fp + eps)
    val_recall = total_tp / (total_tp + total_fn + eps)

    history["val_loss"].append(epoch_val_loss)
    history["val_iou"].append(val_iou)
    history["val_dice"].append(val_dice)
    history["val_precision"].append(val_precision)
    history["val_recall"].append(val_recall)

    print(f"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f},"
          f" Val IoU={val_iou:.4f}, Dice={val_dice:.4f}, Prec={val_precision:.4f}, Rec={val_recall:.4f}")

    # En iyi modeli kaydet
    if val_iou > best_val_iou:
        best_val_iou = val_iou
        torch.save(model.state_dict(), "best_segformer_model_256.pth")
        print("  Saved best model.")

    scheduler.step()

print("Training complete. Best Val IoU:", best_val_iou)

# 12. (İsteğe bağlı) Test setinde değerlendirme
model.load_state_dict(torch.load("best_segformer_model_256.pth"))
model.eval()
test_tp = test_fp = test_fn = 0
test_loss_total = 0.0
with torch.no_grad():
    for pixel_values, labels in tqdm(test_loader, desc="Test Evaluation"):
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)
        outputs = model(pixel_values=pixel_values)
        logits = outputs.logits
        loss = criterion(logits, labels)
        test_loss_total += loss.item() * pixel_values.size(0)

        preds = torch.argmax(logits, dim=1)
        preds_flat = preds.view(-1)
        labels_flat = labels.view(-1)
        tp = ((preds_flat == 1) & (labels_flat == 1)).sum().item()
        fp = ((preds_flat == 1) & (labels_flat == 0)).sum().item()
        fn = ((preds_flat == 0) & (labels_flat == 1)).sum().item()
        test_tp += tp
        test_fp += fp
        test_fn += fn

test_loss = test_loss_total / len(test_dataset)
eps = 1e-6
test_iou = test_tp / (test_tp + test_fp + test_fn + eps)
test_dice = 2 * test_tp / (2 * test_tp + test_fp + test_fn + eps)
test_prec = test_tp / (test_tp + test_fp + eps)
test_rec = test_tp / (test_tp + test_fn + eps)
print(f"Test Loss={test_loss:.4f}, IoU={test_iou:.4f}, Dice={test_dice:.4f}, Prec={test_prec:.4f}, Rec={test_rec:.4f}")

# EN SON BUNU YAZDIM
# 2. Importlar ve GPU kontrolü
import os
import random
import numpy as np
import pandas as pd
from glob import glob
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Cihaz
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Colab + Drive kullanılıyorsa:
from google.colab import drive
drive.mount('/content/drive')

# 3. Yol tanımları (ortamınıza göre ayarlayın)
csv_path = '/content/drive/MyDrive/BubbleSheetScannerProject/labeled_data.csv'
image_dir = '/content/drive/MyDrive/BubbleSheetScannerProject/Allimages/'
mask_dir = '/content/drive/MyDrive/BubbleSheetScannerProject/masks/'

# 4. CSV okuma ve dosya varlığı kontrolü
data = pd.read_csv(csv_path)
filenames = data['filename'].tolist()
image_paths = [os.path.join(image_dir, fn) for fn in filenames]
mask_paths = [os.path.join(mask_dir, fn) for fn in filenames]

paired = [(img, msk) for img, msk in zip(image_paths, mask_paths) if os.path.isfile(img) and os.path.isfile(msk)]
if len(paired) == 0:
    raise RuntimeError("Hiç geçerli image-mask çifti bulunamadı; yol ve dosya adlarını kontrol edin.")
image_paths, mask_paths = zip(*paired)
image_paths = list(image_paths)
mask_paths = list(mask_paths)
print(f"Total valid image-mask pairs: {len(image_paths)}")

# 5. Train/Val/Test bölme
train_imgs, valtest_imgs, train_masks, valtest_masks = train_test_split(
    image_paths, mask_paths, test_size=0.30, random_state=42
)
val_imgs, test_imgs, val_masks, test_masks = train_test_split(
    valtest_imgs, valtest_masks, test_size=0.5, random_state=42
)
print(f"Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}")

# 6. Dataset sınıfı: sadece görüntü feature_extractor ile resize+normalize, mask manuel resize
class BubbleSegmentationDataset(Dataset):
    def __init__(self, image_paths, mask_paths, feature_extractor, target_size=(256,256)):
        """
        image_paths: list of image dosya path’leri
        mask_paths: list of mask path’leri
        feature_extractor: sadece görüntü için, resize+normalize yapacak
        target_size: (yükseklik, genişlik), örn. (256,256)
        """
        self.image_paths = image_paths
        self.mask_paths = mask_paths
        self.feature_extractor = feature_extractor
        self.target_size = target_size

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Görüntü oku
        img = Image.open(self.image_paths[idx]).convert("RGB")
        # Mask oku
        mask = Image.open(self.mask_paths[idx]).convert("L")  # grayscale

        # Görüntüyü feature_extractor ile resize+normalize et
        encoding = self.feature_extractor(images=img, return_tensors="pt")
        pixel_values = encoding['pixel_values'].squeeze(0)  # (3, 256,256)

        # Mask’ı nearest ile hedef boyuta getir
        mask_resized = mask.resize(self.target_size[::-1], resample=Image.NEAREST)
        mask_np = np.array(mask_resized)
        mask_bin = (mask_np > 127).astype(np.uint8)
        labels = torch.from_numpy(mask_bin).long()  # (256,256)

        return pixel_values, labels

# 7. Feature extractor ve model initialization
model_name = "nvidia/segformer-b0-finetuned-ade-512-512"
feature_extractor = SegformerFeatureExtractor.from_pretrained(
    model_name,
    size={"height":256, "width":256}
)

# İki sınıflı segmentasyon için
id2label = {0: "background", 1: "bubble"}
label2id = {"background": 0, "bubble": 1}

model = SegformerForSemanticSegmentation.from_pretrained(
    model_name,
    num_labels=2,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)
model.to(device)

# 8. DataLoader
batch_size = 4  # GPU belleğine göre ayarlayın
train_dataset = BubbleSegmentationDataset(train_imgs, train_masks, feature_extractor, target_size=(256,256))
val_dataset = BubbleSegmentationDataset(val_imgs, val_masks, feature_extractor, target_size=(256,256))
test_dataset = BubbleSegmentationDataset(test_imgs, test_masks, feature_extractor, target_size=(256,256))

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

# 9. Kayıp, optimizer, scheduler
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# 10. Metrik fonksiyonu
def compute_metrics(preds, labels):
    """
    preds: torch.Tensor (N,H,W) değerler 0/1
    labels: torch.Tensor (N,H,W) değerler 0/1
    Pozitif sınıf (1) için IoU, Dice, Precision, Recall, F1 ve Accuracy döner.
    """
    preds = preds.view(-1)
    labels = labels.view(-1)
    tp = ((preds == 1) & (labels == 1)).sum().item()
    tn = ((preds == 0) & (labels == 0)).sum().item()
    fp = ((preds == 1) & (labels == 0)).sum().item()
    fn = ((preds == 0) & (labels == 1)).sum().item()
    total = preds.numel()
    correct = tp + tn
    eps = 1e-6
    iou = tp / (tp + fp + fn + eps)
    dice = 2 * tp / (2 * tp + fp + fn + eps)
    precision = tp / (tp + fp + eps)
    recall = tp / (tp + fn + eps)
    f1 = 2 * precision * recall / (precision + recall + eps)
    accuracy = correct / total
    return {
        "iou": iou,
        "dice": dice,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }

# 11. Eğitim & validation döngüsü (logits upsample ekli)
num_epochs = 20
best_val_iou = 0.0
history = {"train_loss": [], "val_loss": [], "val_iou": [], "val_dice": [], "val_precision": [], "val_recall": []}

for epoch in range(num_epochs):
    # Eğitim
    model.train()
    running_loss = 0.0
    loop = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Training")
    for pixel_values, labels in loop:
        pixel_values = pixel_values.to(device)    # (B,3,256,256)
        labels = labels.to(device)                # (B,256,256)
        optimizer.zero_grad()
        outputs = model(pixel_values=pixel_values)
        logits = outputs.logits  # (B, num_labels=2, H_out, W_out), örn. (B,2,64,64)

        # Logits'i label boyutuna upsample et
        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)
        # Şimdi logits shape: (B,2,256,256)

        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * pixel_values.size(0)
        loop.set_postfix(loss=loss.item())

    epoch_train_loss = running_loss / len(train_dataset)
    history["train_loss"].append(epoch_train_loss)

    # Validation
    model.eval()
    val_running_loss = 0.0
    total_tp = total_fp = total_fn = 0
    with torch.no_grad():
        for pixel_values, labels in tqdm(val_loader, desc=f"Epoch [{epoch+1}/{num_epochs}] Validation"):
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)
            outputs = model(pixel_values=pixel_values)
            logits = outputs.logits  # (B,2,H_out,W_out)
            logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)
            loss = criterion(logits, labels)
            val_running_loss += loss.item() * pixel_values.size(0)

            preds = torch.argmax(logits, dim=1)  # (B,256,256)
            preds_flat = preds.view(-1)
            labels_flat = labels.view(-1)
            tp = ((preds_flat == 1) & (labels_flat == 1)).sum().item()
            fp = ((preds_flat == 1) & (labels_flat == 0)).sum().item()
            fn = ((preds_flat == 0) & (labels_flat == 1)).sum().item()
            total_tp += tp
            total_fp += fp
            total_fn += fn

    epoch_val_loss = val_running_loss / len(val_dataset)
    eps = 1e-6
    val_iou = total_tp / (total_tp + total_fp + total_fn + eps)
    val_dice = 2 * total_tp / (2 * total_tp + total_fp + total_fn + eps)
    val_precision = total_tp / (total_tp + total_fp + eps)
    val_recall = total_tp / (total_tp + total_fn + eps)
    history["val_loss"].append(epoch_val_loss)
    history["val_iou"].append(val_iou)
    history["val_dice"].append(val_dice)
    history["val_precision"].append(val_precision)
    history["val_recall"].append(val_recall)

    print(f"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f},"
          f" Val IoU={val_iou:.4f}, Dice={val_dice:.4f}, Prec={val_precision:.4f}, Rec={val_recall:.4f}")

    if val_iou > best_val_iou:
        best_val_iou = val_iou
        torch.save(model.state_dict(), "best_segformer_model_256.pth")
        print("  Saved best model.")
    scheduler.step()

print("Training complete. Best Val IoU:", best_val_iou)

# 12. Test setinde değerlendirme (aynı upsample adımı ile)
model.load_state_dict(torch.load("best_segformer_model_256.pth"))
model.eval()
test_tp = test_fp = test_fn = 0
test_loss_total = 0.0
with torch.no_grad():
    for pixel_values, labels in tqdm(test_loader, desc="Test Evaluation"):
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)
        outputs = model(pixel_values=pixel_values)
        logits = outputs.logits
        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)
        loss = criterion(logits, labels)
        test_loss_total += loss.item() * pixel_values.size(0)

        preds = torch.argmax(logits, dim=1)
        preds_flat = preds.view(-1)
        labels_flat = labels.view(-1)
        tp = ((preds_flat == 1) & (labels_flat == 1)).sum().item()
        fp = ((preds_flat == 1) & (labels_flat == 0)).sum().item()
        fn = ((preds_flat == 0) & (labels_flat == 1)).sum().item()
        test_tp += tp
        test_fp += fp
        test_fn += fn

test_loss = test_loss_total / len(test_dataset)
eps = 1e-6
test_iou = test_tp / (test_tp + test_fp + test_fn + eps)
test_dice = 2 * test_tp / (2 * test_tp + test_fp + test_fn + eps)
test_prec = test_tp / (test_tp + test_fp + eps)
test_rec = test_tp / (test_tp + test_fn + eps)
test_f1= 2 *test_prec * test_rec / (test_prec + test_rec + eps)
print(f"Test Loss={test_loss:.4f}, IoU={test_iou:.4f}, Dice={test_dice:.4f}, Prec={test_prec:.4f}, Rec={test_rec:.4f}, F1={test_f1:.4f}")

# 12. Test setinde değerlendirme (accuracy dahil)
model.load_state_dict(torch.load("best_segformer_model_256.pth"))
model.eval()
test_tp = test_fp = test_fn = test_tn = 0
test_loss_total = 0.0
with torch.no_grad():
    for pixel_values, labels in tqdm(test_loader, desc="Test Evaluation"):
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)
        outputs = model(pixel_values=pixel_values)
        logits = outputs.logits
        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)
        loss = criterion(logits, labels)
        test_loss_total += loss.item() * pixel_values.size(0)

        preds = torch.argmax(logits, dim=1)
        preds_flat = preds.view(-1)
        labels_flat = labels.view(-1)
        tp = ((preds_flat == 1) & (labels_flat == 1)).sum().item()
        fp = ((preds_flat == 1) & (labels_flat == 0)).sum().item()
        fn = ((preds_flat == 0) & (labels_flat == 1)).sum().item()
        tn = ((preds_flat == 0) & (labels_flat == 0)).sum().item()
        test_tp += tp
        test_fp += fp
        test_fn += fn
        test_tn += tn

test_loss = test_loss_total / len(test_dataset)
eps = 1e-6
total = test_tp + test_fp + test_fn + test_tn
test_iou = test_tp / (test_tp + test_fp + test_fn + eps)
test_dice = 2 * test_tp / (2 * test_tp + test_fp + test_fn + eps)
test_prec = test_tp / (test_tp + test_fp + eps)
test_rec = test_tp / (test_tp + test_fn + eps)
test_f1 = 2 * test_prec * test_rec / (test_prec + test_rec + eps)
test_acc = (test_tp + test_tn) / total

print(f"Test Loss={test_loss:.4f}, IoU={test_iou:.4f}, Dice={test_dice:.4f}, "
      f"Prec={test_prec:.4f}, Rec={test_rec:.4f}, F1={test_f1:.4f}, Acc={test_acc:.4f}")

# 12. Test setinde değerlendirme (aynı upsample adımı ile)
model.load_state_dict(torch.load("best_segformer_model_256.pth"))
model.eval()
test_tp = test_fp = test_fn = 0
test_loss_total = 0.0
with torch.no_grad():
    for pixel_values, labels in tqdm(test_loader, desc="Test Evaluation"):
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)
        outputs = model(pixel_values=pixel_values)
        logits = outputs.logits
        logits = F.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)
        loss = criterion(logits, labels)
        test_loss_total += loss.item() * pixel_values.size(0)

        preds = torch.argmax(logits, dim=1)
        preds_flat = preds.view(-1)
        labels_flat = labels.view(-1)
        tp = ((preds_flat == 1) & (labels_flat == 1)).sum().item()
        fp = ((preds_flat == 1) & (labels_flat == 0)).sum().item()
        fn = ((preds_flat == 0) & (labels_flat == 1)).sum().item()
        test_tp += tp
        test_fp += fp
        test_fn += fn

test_loss = test_loss_total / len(test_dataset)
eps = 1e-6
test_iou = test_tp / (test_tp + test_fp + test_fn + eps)
test_dice = 2 * test_tp / (2 * test_tp + test_fp + test_fn + eps)
test_prec = test_tp / (test_tp + test_fp + eps)
test_rec = test_tp / (test_tp + test_fn + eps)
test_f1= 2 *test_prec * test_rec / (test_prec + test_rec + eps)
print(f"Test Loss={test_loss:.4f}, IoU={test_iou:.4f}, Dice={test_dice:.4f}, Prec={test_prec:.4f}, Rec={test_rec:.4f}, F1={test_f1:.4f}")

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

def visualize_examples(model, feature_extractor, dataset, device, indices):
    """
    model: eğitilmiş SegFormer modeli (SegformerForSemanticSegmentation)
    feature_extractor: SegformerFeatureExtractor (input resize+normalize için)
    dataset: BubbleSegmentationDataset örneği
    device: 'cuda' veya 'cpu'
    indices: görselleştirmek istediğiniz örneklerin dataset içindeki indeksleri (list of int)
    """
    model.eval()
    n = len(indices)
    # Görüntüleri tek tek alıp görselleştireceğiz.
    plt.figure(figsize=(12, 4 * n))
    for i, idx in enumerate(indices):
        # 1. Veri al
        pixel_values, true_mask = dataset[idx]
        # pixel_values: torch.Tensor (3,256,256), true_mask: torch.LongTensor (256,256)
        # orijinal görüntüyü yeniden elde etmek istiyorsanız, dataset içinde ham path'ten PIL açmanız gerekebilir.
        # Burada varsayıyoruz ki dataset içinde image_paths erişilebilir:
        try:
            img_path = dataset.image_paths[idx]
            orig_img = Image.open(img_path).convert("RGB").resize((256,256), resample=Image.BILINEAR)
            orig_np = np.array(orig_img)
        except:
            # Eğer dataset sadece tensor tutuyorsa, pixel_values'tan geri döndürme:
            # pixel_values normalize edilmiş olabilir; tam geri dönüştürme zor.
            orig_np = None

        # 2. Model tahmini
        with torch.no_grad():
            x = pixel_values.unsqueeze(0).to(device)  # (1,3,256,256)
            outputs = model(pixel_values=x)
            logits = outputs.logits  # (1,2,H_out,W_out), örn. (1,2,64,64)
            # Up-sample logits’i 256×256’ya:
            logits = F.interpolate(logits, size=true_mask.shape, mode='bilinear', align_corners=False)
            pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)  # (256,256)

        true_np = true_mask.numpy().astype(np.uint8)  # (256,256)

        # Eğer orig_np yoksa, pixel_values’dan bir tahmini görsel elde etmek isteyebilirsiniz:
        if orig_np is None:
            # pixel_values normalizasyon içeriyorsa, gösterim için uygula:
            img_tensor = pixel_values.cpu().permute(1,2,0).numpy()  # (256,256,3)
            # Eğer feature_extractor renormalize yaptıysa, geri dönüştürmek için mean/std bilgisine ihtiyacınız var.
            # Burada basitçe [0,1] aralığında varsayalım:
            orig_np = (img_tensor - img_tensor.min())/(img_tensor.max()-img_tensor.min()) * 255.0
            orig_np = orig_np.astype(np.uint8)

        # 3. Görselleştirme: 4 sütun: Orijinal / True Mask / Pred Mask / Overlay
        # Her biri alt grafik
        row = i * 4
        # Orijinal
        plt.subplot(n, 4, row+1)
        plt.imshow(orig_np)
        plt.axis('off')
        if i==0: plt.title("Original")

        # True mask: 0/1; gösterirken grayscale
        plt.subplot(n, 4, row+2)
        plt.imshow(true_np, cmap='gray')
        plt.axis('off')
        if i==0: plt.title("True Mask")

        # Predicted mask
        plt.subplot(n, 4, row+3)
        plt.imshow(pred, cmap='gray')
        plt.axis('off')
        if i==0: plt.title("Predicted Mask")

        # Overlay: orijinal üzerine true ve predicted ayrı ayrı overlay yapmak mümkün.
        # Burada örnek: gerçek mask overlay
        plt.subplot(n, 4, row+4)
        plt.imshow(orig_np)
        # Şeffaflıkla mask gölgesi: mask=1 alanlarda alpha=0.3
        # matplotlib default colormap ile viridis tarzı renk olabilir.
        plt.imshow(true_np, alpha=0.3)
        plt.axis('off')
        if i==0: plt.title("Overlay  Mask")

        # Eğer isterseniz predicted overlay için ayrı plot ekleyebilirsiniz.
        # Bu şablonu genişleterek 5. sütunda predicted overlay de ekleyebilirsiniz.

    plt.tight_layout()
    plt.show()

# Kullanım örneği:
# Cihaz ayarı:
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Modeli yükleyin:
# model.load_state_dict(torch.load("best_segformer_model_256.pth"))
model.to(device)

# Dataset tanımınızda image_paths tutuluyorsa:
# test_dataset = BubbleSegmentationDataset(test_imgs, test_masks, feature_extractor, target_size=(256,256))

# Üç örnek indeks seçin. Örneğin ilk üç:
example_indices = [0, 1, 2]
# Veya random:
# import random; example_indices = random.sample(range(len(test_dataset)), 3)

visualize_examples(model, feature_extractor, test_dataset, device, example_indices)

# 1. Gerekli kütüphanelerin yüklenmesi
!pip install transformers==4.35.0 torch torchvision --quiet
!pip install timm datasets --quiet
!pip install --upgrade huggingface_hub transformers --quiet

# 2. İmportlar ve GPU kontrolü
import os
import numpy as np
import pandas as pd
from PIL import Image
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

from google.colab import drive
drive.mount('/content/drive')

# 3. Yol tanımları
csv_path  = '/content/drive/MyDrive/BubbleSheetScannerProject/labeled_data.csv'
image_dir = '/content/drive/MyDrive/BubbleSheetScannerProject/Allimages/'
mask_dir  = '/content/drive/MyDrive/BubbleSheetScannerProject/masks/'
SAVE_DIR  = '/content/drive/MyDrive/BubbleSheetScannerProject/models/'
os.makedirs(SAVE_DIR, exist_ok=True)

# 4. CSV oku ve var olan çiftleri filtrele
df = pd.read_csv(csv_path)
files = df['filename'].tolist()
imgs  = [os.path.join(image_dir, fn) for fn in files]
msks  = [os.path.join(mask_dir,  fn) for fn in files]
paired = [(i,m) for i,m in zip(imgs,msks) if os.path.isfile(i) and os.path.isfile(m)]
if not paired:
    raise RuntimeError("Geçerli image-mask çifti yok")
imgs, msks = zip(*paired)
imgs, msks = list(imgs), list(msks)
print(f"Total pairs: {len(imgs)}")

# 5. %70/%15/%15 split
tr_i, valtest_i, tr_m, valtest_m = train_test_split(imgs, msks, test_size=0.30, random_state=42)
val_i, te_i,  val_m,  te_m      = train_test_split(valtest_i, valtest_m, test_size=0.50, random_state=42)
print(f"Train: {len(tr_i)}, Val: {len(val_i)}, Test: {len(te_i)}")

class BubbleDS(Dataset):
    def __init__(self, imgs, msks, fe):
        self.imgs, self.msks, self.fe = imgs, msks, fe

    def __len__(self): return len(self.imgs)

    def __getitem__(self, idx):
        img = Image.open(self.imgs[idx]).convert("RGB")
        msk = Image.open(self.msks[idx]).convert("L")
        mnp = (np.array(msk) > 127).astype(np.uint8)

        # image: PIL, mask: numpy → feature extractor çağrısı
        encoding = self.fe(images=img, semantic_segmentation_maps=mnp, return_tensors="pt")
        pv = encoding["pixel_values"].squeeze(0)  # (3,256,256)
        label = encoding["labels"].squeeze(0).long()  # (256,256)

        return pv, label


# 7. Feature extractor & model
model_name = "nvidia/segformer-b0-finetuned-ade-512-512"
fe = SegformerFeatureExtractor.from_pretrained(model_name,
    size={"height":256,"width":256})
model = SegformerForSemanticSegmentation.from_pretrained(
    model_name, num_labels=2, ignore_mismatched_sizes=True
).to(device)

# 8. Dataloaders
bs = 4
train_dl = DataLoader(BubbleDS(tr_i, tr_m, fe), batch_size=bs, shuffle=True,  num_workers=2)
val_dl   = DataLoader(BubbleDS(val_i,val_m, fe), batch_size=bs, shuffle=False, num_workers=2)
test_dl  = DataLoader(BubbleDS(te_i, te_m, fe), batch_size=bs, shuffle=False, num_workers=2)

# 9. Kayıp, optimizer, scheduler
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# 10. Metrik fonksiyonu (accuracy, prec, rec, f1, dice, iou)
def compute_metrics(preds, labels):
    p = preds.view(-1)
    t = labels.view(-1)
    tp = ((p==1)&(t==1)).sum().item()
    fp = ((p==1)&(t==0)).sum().item()
    fn = ((p==0)&(t==1)).sum().item()
    tn = ((p==0)&(t==0)).sum().item()
    eps = 1e-6
    acc = (tp+tn)/(tp+tn+fp+fn+eps)
    prec= tp/(tp+fp+eps)
    rec = tp/(tp+fn+eps)
    f1  = 2*prec*rec/(prec+rec+eps)
    iou = tp/(tp+fp+fn+eps)
    dice= 2*tp/(2*tp+fp+fn+eps)
    return {"loss": None, "accuracy":acc, "precision":prec,
            "recall":rec, "f1":f1, "iou":iou, "dice":dice}

# 11. Eğitim + EarlyStopping(val_iou)
best_val_iou = 0.0
patience, wait = 5, 0
history = []

for ep in range(1,21):
    # train
    model.train()
    train_loss = 0.0
    for X,Y in tqdm(train_dl, desc=f"Train {ep}"):
        X, Y = X.to(device), Y.to(device)
        optimizer.zero_grad()
        out = model(pixel_values=X).logits
        loss= criterion(out, Y)
        loss.backward(); optimizer.step()
        train_loss += loss.item()*X.size(0)
    train_loss /= len(train_dl.dataset)

    # val
    model.eval()
    val_loss = 0.0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X,Y in tqdm(val_dl, desc=f"Val {ep}"):
            X, Y = X.to(device), Y.to(device)
            out = model(pixel_values=X).logits
            val_loss += criterion(out, Y).item()*X.size(0)
            preds = torch.argmax(out, dim=1)
            all_preds.append(preds.cpu())
            all_labels.append(Y.cpu())
    val_loss /= len(val_dl.dataset)
    preds_cat = torch.cat(all_preds)
    labels_cat= torch.cat(all_labels)
    m = compute_metrics(preds_cat, labels_cat)
    m["loss"]= val_loss

    print(f"Epoch {ep}: Train Loss={train_loss:.4f} — Val Loss={val_loss:.4f}, " +
          f"IoU={m['iou']:.4f}, Dice={m['dice']:.4f}, Prec={m['precision']:.4f}, " +
          f"Rec={m['recall']:.4f}, F1={m['f1']:.4f}")

    # EarlyStopping & Checkpoint
    if m["iou"] > best_val_iou + 1e-6:
        best_val_iou = m["iou"]
        wait = 0
        torch.save(model.state_dict(), os.path.join(SAVE_DIR,"best_segformer.pth"))
        print("  ▶ Saved best model.")
    else:
        wait += 1
        if wait >= patience:
            print("Early stopping triggered.")
            break
    scheduler.step()

print(f"Training done, best val IoU = {best_val_iou:.4f}")

# 12. Test Değerlendirme
model.load_state_dict(torch.load(os.path.join(SAVE_DIR,"best_segformer.pth")))
model.eval()
all_preds, all_labels = [], []; test_loss = 0.0
with torch.no_grad():
    for X,Y in tqdm(test_dl, desc="Test Eval"):
        X, Y = X.to(device), Y.to(device)
        out = model(pixel_values=X).logits
        test_loss += criterion(out, Y).item()*X.size(0)
        preds = torch.argmax(out, dim=1)
        all_preds.append(preds.cpu()); all_labels.append(Y.cpu())
test_loss /= len(test_dl.dataset)
preds_cat = torch.cat(all_preds); labels_cat = torch.cat(all_labels)
tm = compute_metrics(preds_cat, labels_cat)
tm["loss"] = test_loss

print("\n--- Test Metrics ---")
print(f"Loss: {tm['loss']:.4f}, Acc: {tm['accuracy']:.4f}, Prec: {tm['precision']:.4f}, " +
      f"Rec: {tm['recall']:.4f}, F1: {tm['f1']:.4f}, Dice: {tm['dice']:.4f}, IoU: {tm['iou']:.4f}")