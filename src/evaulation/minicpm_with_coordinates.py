# -*- coding: utf-8 -*-
"""MiniCPM-FULL-PostProc.ipynb adlı dosyanın kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ksXU9rIx7upObb1LIba66ZYnmRt6ZTiV
"""

# full_coords_inference_with_postproc.py
# Inference script for Full+coords condition (no crop image, only full image + bbox-aware prompt)
# Added preprocessing & postprocessing utilities (from the no-crop script) such as:
# - normalize_empty_token, compute_cer, is_repetitive
# - token-level metrics helper token_level_metrics_from_pairs
# - saving detailed results (json + tsv)
#
# Main idea unchanged: we pass the whole image + bbox-aware prompt to the model.

import os
import json
import gc
import torch
import editdistance
import pickle
from PIL import Image, ImageEnhance
from typing import List, Tuple, Optional
from transformers import AutoModel, AutoTokenizer
import numpy as np
from tqdm import tqdm
from collections import defaultdict
from sklearn.metrics import precision_recall_fscore_support
import matplotlib.pyplot as plt

# For BLEU (optional)
try:
    from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
    _HAS_NLTK = True
except Exception:
    _HAS_NLTK = False

# If running in Colab, uncomment:
from google.colab import drive
drive.mount('/content/drive')

# --------------------
# Config paths
# --------------------
ROOT = os.environ.get("ALIGNED_SHEETS_ROOT", "/content/drive/MyDrive/Aligned_Sheets")
JSON_PATH = os.path.join(ROOT, "dataset_updated.json")
OUTPUT_DIR = os.path.join(ROOT, "inference_outputs_full_coords_with_postproc")
os.makedirs(OUTPUT_DIR, exist_ok=True)

MODEL_ID = "openbmb/MiniCPM-V-4_5"
# adjust dtype if needed
TORCH_DTYPE = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float32
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"[i] ROOT={ROOT}")
print(f"[i] Using device={DEVICE}, dtype={TORCH_DTYPE}, model={MODEL_ID}")

# --------------------
# Load model
# --------------------
print("[i] Loading MiniCPM model & tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True, torch_dtype=TORCH_DTYPE).eval().to(DEVICE)
print("[i] Model loaded.")

# --------------------
# --- PRE/POST-PROCESSING HELPERS (integrated)
# --------------------

def preprocess_image(img: Image.Image) -> Image.Image:
    """Enhance contrast/sharpness a bit (keeps same idea as before)."""
    enhancer = ImageEnhance.Sharpness(img)
    img = enhancer.enhance(1.5)
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(1.2)
    return img

def normalize_empty_token(s: Optional[str]) -> str:
    if s is None or s == "":
        return "<EMPTY>"
    return s

def compute_cer(pred: str, gold: str) -> float:
    pred_n = normalize_empty_token(pred)
    gold_n = normalize_empty_token(gold)
    if gold_n is None or len(gold_n) == 0:
        return float(len(pred_n) > 0)
    # protect division by zero
    return editdistance.eval(pred_n, gold_n) / max(1, len(gold_n))

def is_repetitive(seq: str, min_repeats_factor: int = 6) -> bool:
    if not seq:
        return False
    if len(set(seq)) == 1 and len(seq) > 4:
        return True
    for k in range(1,7):
        if len(seq) > k * min_repeats_factor:
            sub = seq[:k]
            if sub * (len(seq)//k) == seq[:(len(seq)//k)*k]:
                return True
    return False

def token_level_metrics_from_pairs(pairs: List[Tuple[str,str]], pad_token: str = '<PAD>'):
    def to_token_list(s: str):
        if s == "<EMPTY>":
            return ['\u0001']
        return list(s)

    y_true = []
    y_pred = []
    classes = set()
    for gold, pred in pairs:
        if gold is None or gold == "":
            gold = "<EMPTY>"
        if pred is None or pred == "":
            pred = "<EMPTY>"
        gold_tokens = to_token_list(gold)
        pred_tokens = to_token_list(pred)
        maxl = max(len(gold_tokens), len(pred_tokens))
        for i in range(maxl):
            gt_tok = gold_tokens[i] if i < len(gold_tokens) else pad_token
            pr_tok = pred_tokens[i] if i < len(pred_tokens) else pad_token
            y_true.append(gt_tok)
            y_pred.append(pr_tok)
            classes.add(gt_tok); classes.add(pr_tok)

    labels = sorted(list(classes))
    total_positions = len(y_true)
    correct = sum(1 for a,b in zip(y_true,y_pred) if a==b)
    token_acc = correct / total_positions if total_positions>0 else 0.0

    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, average='micro', zero_division=0)
    per_label = {}
    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)
    for lbl, pp, rr, ff, ss in zip(labels, p, r, f, s):
        per_label[lbl] = {"precision": float(pp), "recall": float(rr), "f1": float(ff), "support": int(ss)}

    return {
        'token_accuracy': float(token_acc),
        'token_micro_precision': float(precision),
        'token_micro_recall': float(recall),
        'token_micro_f1': float(f1),
        'per_label': per_label,
        'labels': labels,
        'total_positions': total_positions,
        'correct_positions': correct
    }

# --------------------
# Utils for reading dataset & building lookup (robust)
# --------------------

def load_json_entries(path: str):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # keep original filtering logic if needed (only horizontal etc.)
    out = []
    excluded = {'002','004','009','018','020','022','024','025','027','039','040','043'}
    for e in data:
        if e.get('alignment','horizontal') != 'horizontal':
            continue
        p = e.get('path','')
        if p and p[:3] in excluded:
            continue
        out.append(e)
    print(f"[i] Loaded {len(out)} filtered entries from {path}")
    return out

def normalize_answer_val(ans):
    if ans is None:
        return "<EMPTY>"
    if isinstance(ans, list):
        if len(ans) == 0:
            return "<EMPTY>"
        a = ans[0]
    else:
        a = ans
    if a is None:
        return "<EMPTY>"
    a = str(a).strip()
    if a == "":
        return "<EMPTY>"
    return a

def build_lookup_map(entries):
    lookup = {}
    for e in entries:
        image_path = e.get("path")
        if image_path is None:
            continue
        nforms = int(e.get("nforms", 1))
        values = e.get("values", [])
        raw_answers = e.get("answer", [])
        if isinstance(raw_answers, str):
            raw_answers = [[raw_answers]]
        answers = []
        for i in range(nforms):
            try:
                a = raw_answers[i]
            except Exception:
                a = None
            answers.append(normalize_answer_val(a))
        areas = e.get("answer_area", None)
        areas_list = []
        if areas is None:
            areas_list = [None] * nforms
        else:
            if isinstance(areas, list) and len(areas) == 4 and all(isinstance(x, (int, float)) for x in areas):
                areas_list = [tuple(int(x) for x in areas)] + [None] * (nforms - 1)
            else:
                for i in range(nforms):
                    try:
                        a = areas[i]
                        if a is None:
                            areas_list.append(None)
                        else:
                            if isinstance(a, list) and len(a) == 4:
                                areas_list.append(tuple(int(x) for x in a))
                            else:
                                areas_list.append(None)
                    except Exception:
                        areas_list.append(None)
        for form_id in range(nforms):
            gt = answers[form_id] if form_id < len(answers) else "<EMPTY>"
            bbox = areas_list[form_id] if form_id < len(areas_list) else None
            expected_len = len(gt) if gt != "<EMPTY>" else None
            lookup[(image_path, form_id)] = {
                "gt": gt,
                "bbox": bbox,
                "expected_len": expected_len,
                "values": values,
                "nforms": nforms,
            }
    return lookup

# --------------------
# Prompt builder (bbox-aware)
# --------------------
def build_sequence_prompt_from_bbox(bbox: Optional[Tuple[int, int, int, int]], allowed_values: List[str], expected_len: Optional[int] = None) -> str:
    if bbox is None:
        bbox_text = "No bounding box was provided. Read the relevant area visually."
    else:
        (x1, y1, x2, y2) = bbox
        bbox_text = f"The answer region is the rectangle with pixel coordinates (x1,y1,x2,y2)=({x1},{y1},{x2},{y2}), where (0,0) is the top-left of the image."

    allowed_set_str = ",".join([str(x) for x in (allowed_values or [])])
    expected_text = f" The expected answer length is {expected_len} characters." if expected_len is not None else ""

    prompt = (
        "You are given the whole exam page image and coordinates of an answer region. "
        f"{bbox_text} Read ONLY inside that rectangle left-to-right, top-to-bottom and return the concatenated marked answers. "
        f"Allowed answer tokens are: [{allowed_set_str}].{expected_text} "
        "If nothing is marked, return <EMPTY>. Return only the sequence (e.g. ABCD or <EMPTY>) and nothing else."
    )
    return prompt

# --------------------
# Model output extraction (merged & enhanced)
# --------------------
def extract_sequence_from_model_output(raw: Optional[str], allowed_values: List[str], expected_len: Optional[int] = None) -> str:
    """Post-process model output to extract the requested sequence.

    - respects allowed_values when possible (builds allowed char set)
    - returns <EMPTY> for empty / no-match / explicit empty
    - applies repetitive-sequence heuristic and maps to <EMPTY>
    """
    if raw is None:
        return "<EMPTY>"
    text = str(raw).strip()
    if text == "":
        return "<EMPTY>"

    # if model explicitly returned empty token
    if re_search_empty := ("<EMPTY>" in text.upper() or re_search_word_empty(text)):
        return "<EMPTY>"

    # allowed chars from allowed_values (concatenate tokens)
    allowed_chars = set("".join([str(v) for v in (allowed_values or []) if v is not None]))
    # remove angle-bracket token if user included "<EMPTY>" in allowed_values accidentally
    allowed_chars.discard("<"); allowed_chars.discard(">")

    if len(allowed_chars) == 0:
        # fallback: allow alphanumeric
        allowed_chars = set([c for c in text if c.isalnum()])

    # keep order, filter only allowed characters
    filtered = "".join([c for c in text if c in allowed_chars])

    if expected_len is not None:
        filtered = filtered[:expected_len]

    if filtered == "":
        return "<EMPTY>"

    # repetitive check (if returns long repeated junk)
    if is_repetitive(filtered) and filtered != "<EMPTY>":
        return "<EMPTY>"

    return filtered

# small helper to detect "empty" words in raw text (word boundary)
import re
def re_search_word_empty(txt: str) -> bool:
    return bool(re.search(r"\bEMPTY\b", txt, flags=re.IGNORECASE))

# --------------------
# Inference function (Full + coords)
# --------------------
def infer_sequence_full(image_path: str, bbox: Optional[Tuple[int,int,int,int]], allowed_values: List[str], expected_len: Optional[int] = None, max_new_tokens: int = 128):
    full_img = Image.open(image_path).convert("RGB")
    full_img = preprocess_image(full_img)

    prompt = build_sequence_prompt_from_bbox(bbox, allowed_values, expected_len)
    # try the more explicit API (image kw) first
    try:
        msgs = [{"role":"user","content": prompt}]
        pred = model.chat(
            image=full_img,
            msgs=msgs,
            tokenizer=tokenizer,
            sampling=False,
            temperature=0.0,
            max_new_tokens=max_new_tokens,
        )
    except TypeError:
        # fallback style (some envs expect list [image, prompt])
        msgs = [{"role":"user","content":[full_img, prompt]}]
        pred = model.chat(
            image=None,
            msgs=msgs,
            tokenizer=tokenizer,
            sampling=False,
            temperature=0.0,
            max_new_tokens=max_new_tokens,
        )
    # normalize pred to string
    if isinstance(pred, (list,tuple)):
        pred_str = str(pred[0])
    else:
        pred_str = str(pred)

    pred_str = pred_str.strip()
    pred_seq = extract_sequence_from_model_output(pred_str, allowed_values, expected_len)

    return pred_seq

# --------------------
# Metrics & saving helpers
# --------------------
def compute_metrics_and_save(gts_map: dict, preds_map: dict, details: List[dict], out_dir: str):
    """
    Extended metrics & saving:
    - exact match accuracy
    - avg CER
    - token-level metrics (already available)
    - BLEU-1/2/3 (if nltk available)
    - save metrics.json, details.json, details.tsv
    - save plots: eval_summary_bar.png, cer_hist.png, cer_vs_len.png
    """
    os.makedirs(out_dir, exist_ok=True)

    # prepare lists aligned
    keys = sorted(gts_map.keys())
    gt_list = [gts_map[k] for k in keys]
    pred_list = [preds_map.get(k, "<EMPTY>") for k in keys]

    # Exact match accuracy
    exact_matches_arr = [int(g == p) for g,p in zip(gt_list, pred_list)]
    exact_matches = int(sum(exact_matches_arr))
    total_items = len(exact_matches_arr)
    acc = sum(exact_matches_arr) / total_items if total_items > 0 else 0.0

    # CER (per-item and average)
    cer_vals_per_item = []
    total_edits = 0
    total_ref_chars = 0
    for g, p in zip(gt_list, pred_list):
        g_clean = "" if g == "<EMPTY>" else g
        p_clean = "" if p == "<EMPTY>" else p
        ed = editdistance.eval(g_clean, p_clean)
        cer_vals_per_item.append(ed / max(1, len(g_clean)) if len(g_clean)>0 else (1.0 if len(p_clean)>0 else 0.0))
        total_edits += ed
        total_ref_chars += len(g_clean)
    avg_cer = (total_edits / total_ref_chars) if total_ref_chars > 0 else 0.0

    # token-level metrics
    pairs = list(zip(gt_list, pred_list))
    token_metrics = token_level_metrics_from_pairs(pairs)

    # BLEU scores (corpus-level) if available
    bleu1 = bleu2 = bleu3 = 0.0
    if _HAS_NLTK:
        refs = [[list(g)] if g is not None and len(g)>0 else [['']] for g,_ in pairs]
        hyps = [list(p) if p is not None and len(p)>0 else [''] for _,p in pairs]
        refs_fixed = []
        for r in refs:
            if r == [['']] :
                refs_fixed.append([[]])
            else:
                refs_fixed.append(r)
        hyps_fixed = [h if h!=[''] else [] for h in hyps]

        smoothie = SmoothingFunction().method4
        try:
            bleu1 = corpus_bleu(refs_fixed, hyps_fixed, weights=(1,0,0,0), smoothing_function=smoothie)
            bleu2 = corpus_bleu(refs_fixed, hyps_fixed, weights=(0.5,0.5,0,0), smoothing_function=smoothie)
            bleu3 = corpus_bleu(refs_fixed, hyps_fixed, weights=(1/3,1/3,1/3,0), smoothing_function=smoothie)
        except Exception as e:
            print(f"[W] BLEU computation failed: {e}")
            bleu1 = bleu2 = bleu3 = 0.0
    else:
        print("[W] nltk not available: skipping BLEU computations. Install nltk and download punkt if you want BLEU scores.")

    metrics = {
        "total_items": total_items,
        "exact_matches": exact_matches,
        "exact_match_accuracy": acc,
        "avg_cer": avg_cer,
        "cer_values": cer_vals_per_item,
        "cer_sum": total_edits,
        "token_metrics": token_metrics,
        "bleu1": float(bleu1),
        "bleu2": float(bleu2),
        "bleu3": float(bleu3),
    }

    # save metrics
    metrics_path = os.path.join(out_dir, "metrics.json")
    with open(metrics_path, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)

    # save details list (already provided)
    details_path = os.path.join(out_dir, "details.json")
    with open(details_path, "w", encoding="utf-8") as f:
        json.dump(details, f, indent=2, ensure_ascii=False)

    # TSV (tab-separated)
    tsv_path = os.path.join(out_dir, "details.tsv")
    with open(tsv_path, "w", encoding="utf-8") as f:
        f.write("image\tform_id\tgt\tpred\tok\tcer\n")
        for d in details:
            f.write(f"{d['image']}\t{d['form_id']}\t{d['gt']}\t{d['pred']}\t{int(d['ok'])}\t{d['cer']:.6f}\n")

    print(f"[i] Saved metrics -> {metrics_path}")
    print(f"[i] Saved details -> {details_path} and {tsv_path}")

    # --- Visualizations (mirror previous script outputs) ---
    try:
        # Summary bar similar to visualize_metrics
        metrics_for_bar = {
            'Exact-match acc': acc,
            'Token-acc': token_metrics.get('token_accuracy', 0.0),
            'Token-F1 (micro)': token_metrics.get('token_micro_f1', 0.0),
            'BLEU-1': float(bleu1),
            'BLEU-2': float(bleu2),
            'BLEU-3': float(bleu3),
        }
        names = list(metrics_for_bar.keys())
        vals = [metrics_for_bar[n] for n in names]

        plt.figure(figsize=(10,5))
        plt.bar(names, vals)
        plt.ylabel('Score')
        plt.title('Evaluation summary (full_coords_with_postproc)')
        plt.xticks(rotation=20)
        plt.tight_layout()
        summary_bar_path = os.path.join(out_dir, 'eval_summary_bar_fullcoords.png')
        plt.savefig(summary_bar_path)
        plt.close()
        print(f"[i] Saved summary bar to {summary_bar_path}")

        # CER histogram
        cer_vals = [d['cer'] for d in details]
        plt.figure(figsize=(8,5))
        plt.hist(cer_vals, bins=30)
        plt.xlabel('CER')
        plt.ylabel('Count')
        plt.title('CER distribution (full_coords)')
        plt.tight_layout()
        cer_hist_path = os.path.join(out_dir, "cer_hist_fullcoords.png")
        plt.savefig(cer_hist_path)
        plt.close()
        print(f"[i] Saved CER histogram to {cer_hist_path}")

        # CER vs GT length scatter
        lens = [len(d['gt']) if d['gt'] and d['gt'] != "<EMPTY>" else 0 for d in details]
        plt.figure(figsize=(8,5))
        plt.scatter(lens, cer_vals, alpha=0.6)
        plt.xlabel('GT sequence length')
        plt.ylabel('CER')
        plt.title('CER vs GT length (full_coords)')
        plt.tight_layout()
        cer_vs_len_path = os.path.join(out_dir, "cer_vs_len_fullcoords.png")
        plt.savefig(cer_vs_len_path)
        plt.close()
        print(f"[i] Saved CER vs length scatter to {cer_vs_len_path}")

    except Exception as e:
        print(f"[W] plotting failed: {e}")

    return metrics

# --------------------
# Main
# --------------------
if __name__ == "__main__":
    gc.collect()
    entries = load_json_entries(JSON_PATH)
    lookup = build_lookup_map(entries)

    # small test subset (same examples as you used)
    test_examples = [
        ("001/1234567891019.png", 0),
        ("048/1234567891071.png", 1),
        ("005/1234567891699.png", 0),
        ("005/1234567892252.png", 2),
    ]

    print("\n[TEST RUN] Running small subset...")
    for img_rel, form_id in test_examples:
        key = (img_rel, form_id)
        if key not in lookup:
            print(f"[SKIP] {img_rel} form={form_id} not in dataset.")
            continue
        info = lookup[key]
        bbox = info["bbox"]
        gt = info["gt"]
        expected_len = info["expected_len"]
        allowed_values = info.get("values", [])
        image_path = os.path.join(ROOT, img_rel)
        try:
            pred = infer_sequence_full(image_path, bbox, allowed_values, expected_len)
        except Exception as e:
            print(f"[ERROR] {img_rel} form={form_id} -> {e}")
            pred = "<ERROR>"
        print(f"[example_test] image={img_rel} form={form_id} GT={gt} PRED={pred}")

    # FULL BATCH RUN
    print("\n[MAIN RUN] Running full dataset...")
    preds = {}
    gts = {}
    details = []
    for (img_rel, form_id), info in tqdm(lookup.items(), desc="Running inference"):
        bbox = info["bbox"]
        gt = info["gt"]
        expected_len = info["expected_len"]
        allowed_values = info.get("values", [])
        image_path = os.path.join(ROOT, img_rel)
        try:
            pred = infer_sequence_full(image_path, bbox, allowed_values, expected_len)
        except Exception as e:
            print(f"[ERROR] {img_rel} form={form_id} -> {e}")
            pred = "<ERROR>"
        pred_norm = pred if pred is not None else "<EMPTY>"
        gt_norm = gt if gt is not None else "<EMPTY>"
        ok = (gt_norm == pred_norm)
        cer = compute_cer(pred_norm, gt_norm)
        details.append({
            "image": img_rel,
            "form_id": form_id,
            "gt": gt_norm,
            "pred": pred_norm,
            "ok": ok,
            "cer": cer
        })
        preds[(img_rel, form_id)] = pred_norm
        gts[(img_rel, form_id)] = gt_norm
        # append running log
        with open(os.path.join(OUTPUT_DIR, "results_log.txt"), "a", encoding="utf-8") as f:
            f.write(f"image={img_rel} form={form_id} GT={gt_norm} PRED={pred_norm} CER={cer:.6f}\n")

    # save preds
    with open(os.path.join(OUTPUT_DIR, "preds.pkl"), "wb") as f:
        pickle.dump(preds, f)

    # compute metrics & save details
    metrics = compute_metrics_and_save(gts, preds, details, OUTPUT_DIR)
    print("[i] Final metrics:", metrics)
    print("[i] Done. Outputs are in:", OUTPUT_DIR)