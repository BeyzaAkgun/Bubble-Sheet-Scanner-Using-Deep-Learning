# -*- coding: utf-8 -*-
"""minicpm_with_crops.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1izw4jslthce0q_QcyOlCk57a9PIis_KD
"""

#!/usr/bin/env python3

import os
import json
import re
import gc
import torch
import editdistance
import pickle
import csv
from PIL import Image, ImageEnhance
from typing import List, Tuple
from transformers import AutoModel, AutoTokenizer
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from glob import glob

from sklearn.metrics import precision_recall_fscore_support
import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

from google.colab import drive
drive.mount('/content/drive')

# ---------- CONFIG ----------
ROOT = "/content/drive/MyDrive/Aligned_Sheets"   # <-- adjust if needed
JSON_PATH = os.path.join(ROOT, "dataset_updated.json")
OUTPUT_DIR = os.path.join(ROOT, "minicpm_inference_outputs_crops_strict_validator")
os.makedirs(OUTPUT_DIR, exist_ok=True)

VOCAB_PATH = os.path.join(ROOT, "vocab.pkl")

MODEL_ID = "openbmb/MiniCPM-V-4_5-AWQ"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float32

# Updated crop root path to match your structure
CROP_ROOT = os.path.join(ROOT, "cropped_bboxes512_2_padded")

# Fail loudly if any expected crop is missing (set to False to continue and map missing -> "<EMPTY>")
FAIL_ON_MISSING = False  # Set to False to skip missing crops instead of failing

# CSV path for missing crop report
MISSING_CSV_PATH = os.path.join(OUTPUT_DIR, "missing_crops_report.csv")

# Excluded qtypes (same as before)
EXCLUDED_PREFIXES = {'002','004','009','018','020','022','024','025','027','039','040','043'}

print(f"[i] DEVICE={DEVICE}, DTYPE={DTYPE}, MODEL_ID={MODEL_ID}")
print(f"[i] Using CROP_ROOT = {CROP_ROOT}")
print(f"[i] FAIL_ON_MISSING = {FAIL_ON_MISSING}")

# ---------- Vocab ----------
class Vocab:
    def _init_(self):
        self.token2idx = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3, "<EMPTY>": 4}
        self.idx2token = {idx: tk for tk, idx in self.token2idx.items()}
        self.next_idx = 5

    def add_sentence(self, sentence):
        if sentence is None or sentence == "":
            return
        for ch in sentence:
            if ch not in self.token2idx:
                self.token2idx[ch] = self.next_idx
                self.idx2token[self.next_idx] = ch
                self.next_idx += 1

    def _len_(self):
        return len(self.token2idx)

# ---------- JSON helpers ----------
def load_json_entries(json_path: str, excluded_prefixes: set = None) -> List[dict]:
    excluded = excluded_prefixes or EXCLUDED_PREFIXES
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    out = [e for e in data if e.get('alignment','horizontal') == 'horizontal' and e.get('path','')[:3] not in excluded]
    print(f"[i] Loaded {len(out)} filtered entries from {json_path}")
    return out

def build_lookup_map(entries: List[dict]) -> dict:
    return {e['path']: e for e in entries}

def get_gt_for_form(entry: dict, form_id: int):
    answers = entry.get('answer', [])
    if isinstance(answers, list) and form_id < len(answers) and answers[form_id]:
        return answers[form_id][0]
    if isinstance(answers, list) and len(answers) > 0 and answers[0]:
        return answers[0][0]
    return ""

# ---------- Crop validator ----------
def expected_crop_path(entry_path: str, form_id: int) -> str:
    """Generate expected crop path based on your directory structure"""
    qtype = entry_path[:3]
    basename = os.path.splitext(os.path.basename(entry_path))[0]
    fname = f"{basename}_crop{form_id}.png"
    return os.path.join(CROP_ROOT, qtype, fname)

def any_crop_exists_for(entry_path: str) -> bool:
    """Check if any crop exists for the given entry"""
    qtype = entry_path[:3]
    basename = os.path.splitext(os.path.basename(entry_path))[0]
    folder = os.path.join(CROP_ROOT, qtype)
    if not os.path.isdir(folder):
        return False
    matches = glob(os.path.join(folder, f"{basename}_crop*.png"))
    return len(matches) > 0

def get_available_crops_for_entry(entry_path: str) -> List[int]:
    """Get list of available crop IDs for an entry (0, 1, 2, 3)"""
    qtype = entry_path[:3]
    basename = os.path.splitext(os.path.basename(entry_path))[0]
    folder = os.path.join(CROP_ROOT, qtype)
    if not os.path.isdir(folder):
        return []

    available_crops = []
    for crop_id in range(4):  # crop0, crop1, crop2, crop3
        crop_path = os.path.join(folder, f"{basename}_crop{crop_id}.png")
        if os.path.isfile(crop_path):
            available_crops.append(crop_id)
    return available_crops

def validate_and_report(entries: List[dict], save_csv: bool = True) -> List[Tuple[str,int,str]]:
    """
    Returns list of missing crops as tuples: (image_path, form_id, expected_fullpath)
    Also writes CSV to MISSING_CSV_PATH when save_csv is True.
    """
    missing = []
    for e in entries:
        p = e['path']
        # determine number of forms
        nforms = e.get('nforms', len(e.get('answer_area', [])))
        try:
            nforms = int(nforms)
        except Exception:
            nforms = len(e.get('answer_area', []))

        # Check available crops for this entry
        available_crops = get_available_crops_for_entry(p)

        # collect expected
        for fid in range(nforms):
            exp = expected_crop_path(p, fid)
            if not os.path.isfile(exp):
                missing.append((p, fid, exp))

    # Also check for images with zero crops at all (sanity)
    no_crop_images = [e['path'] for e in entries if not any_crop_exists_for(e['path'])]

    # Summary statistics
    total_expected = sum(e.get('nforms', len(e.get('answer_area', []))) for e in entries)
    total_available = sum(len(get_available_crops_for_entry(e['path'])) for e in entries)

    print(f"[i] Total entries checked: {len(entries)}")
    print(f"[i] Total expected crops: {total_expected}")
    print(f"[i] Total available crops: {total_available}")
    print(f"[i] Missing crop files (count): {len(missing)}")
    print(f"[i] Images with zero crops found under {CROP_ROOT}: {len(no_crop_images)}")

    if save_csv:
        with open(MISSING_CSV_PATH, 'w', newline='', encoding='utf-8') as csvf:
            writer = csv.writer(csvf)
            writer.writerow(["image_path","form_id","expected_crop_path","status"])
            for r in missing:
                writer.writerow([r[0], r[1], r[2], "missing"])

            # Write available crops summary
            writer.writerow([])
            writer.writerow(["# Available crops per entry"])
            writer.writerow(["image_path","available_crop_ids","count"])
            for e in entries[:20]:  # Show first 20 as examples
                available = get_available_crops_for_entry(e['path'])
                writer.writerow([e['path'], str(available), len(available)])

            # optionally write images with zero crops section
            writer.writerow([])
            writer.writerow(["images_with_zero_crops"])
            for im in no_crop_images:
                writer.writerow([im])
        print(f"[i] Missing crops report written to {MISSING_CSV_PATH}")

    return missing

# ---------- Image helpers (CROPS ONLY) ----------
def load_crop_image_strict(entry_path: str, form_id: int, debug: bool=False) -> Image.Image:
    """
    Strictly loads the pre-cropped image from CROP_ROOT/qtype/<basename>_crop{form_id}.png
    Returns PIL.Image or None if not found.
    """
    qtype = entry_path[:3]
    basename = os.path.splitext(os.path.basename(entry_path))[0]
    folder_q = os.path.join(CROP_ROOT, qtype)

    if not os.path.isdir(folder_q):
        if debug:
            print(f"[W] Crop folder for qtype not found: {folder_q}")
        return None

    exact_path = os.path.join(folder_q, f"{basename}_crop{form_id}.png")
    if os.path.isfile(exact_path):
        try:
            img = Image.open(exact_path).convert("RGB")
            img = ImageEnhance.Sharpness(img).enhance(1.0)
            if debug:
                print(f"[i] Loaded crop: {exact_path} (size: {img.size})")
            return img
        except Exception as e:
            if debug:
                print(f"[E] Failed to open crop image {exact_path}: {e}")
            return None

    # Fallback tolerant search for small naming quirks
    candidates = glob(os.path.join(folder_q, f"{basename}crop{form_id}*")) + glob(os.path.join(folder_q, f"{basename}_crop{form_id}*"))
    candidates = sorted(set(candidates))
    if candidates:
        try:
            img = Image.open(candidates[0]).convert("RGB")
            img = ImageEnhance.Sharpness(img).enhance(1.0)
            if debug:
                print(f"[i] Loaded crop (fallback): {candidates[0]} (size: {img.size})")
            return img
        except Exception as e:
            if debug:
                print(f"[E] Failed to open tolerant-candidate {candidates[0]}: {e}")
            return None

    if debug:
        print(f"[W] No crop found strictly for {entry_path} form {form_id} in {folder_q}")
    return None

# ---------- Model load ----------
def load_model_and_tokenizer(model_id: str, dtype, device):
    print("[i] Loading MiniCPM model & tokenizer...")
    try:
        model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=dtype).eval().to(device)
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        print(f"[i] Loaded model {model_id} on {device}, dtype={dtype}")
    except Exception as e:
        print(f"[W] Primary model load failed: {e}. Trying fallback model id.")
        MODEL_ID_FALLBACK = "openbmb/MiniCPM-V-4_5"
        model = AutoModel.from_pretrained(MODEL_ID_FALLBACK, trust_remote_code=True, torch_dtype=dtype).eval().to(device)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID_FALLBACK, trust_remote_code=True)
        print(f"[i] Loaded fallback model {MODEL_ID_FALLBACK} on {device}")
    return model, tokenizer

# ---------- Vocab load/build ----------
def load_or_build_vocab(vocab_path: str, json_path: str):
    if os.path.exists(vocab_path):
        with open(vocab_path, "rb") as f:
            vocab = pickle.load(f)
        print(f"[i] Loaded vocab from {vocab_path}, size={len(vocab)}")
    else:
        print("[i] Building dataset-wide vocab from dataset answers...")
        entries_tmp = load_json_entries(json_path)
        vocab = Vocab()
        for e in entries_tmp:
            for ans_list in e.get("answer", []):
                if ans_list and len(ans_list) > 0:
                    vocab.add_sentence(ans_list[0])
        with open(vocab_path, "wb") as f:
            pickle.dump(vocab, f)
        print(f"[i] Built and saved vocab to {vocab_path}, size={len(vocab)}")
    return vocab

# ---------- Prompt builder for crop ----------
def build_sequence_prompt_for_crop(valid_values: List[str], expected_len: int = None) -> str:
    vals = ' '.join(valid_values)
    exp_text = f"Expected length = {expected_len}. Return EXACTLY this many symbols if possible. " if expected_len else ""
    prompt = (
        f"You are given a cropped image (single bubble-form field). "
        f"The valid symbols that can appear are exactly: {vals}. "
        f"{exp_text}"
        f"Read the crop left-to-right, top-to-bottom. "
        f"If no marked bubbles are visible for the requested field, reply with the single token: <EMPTY> (including angle brackets). "
        f"Return ONLY the concatenated sequence of symbols for marked bubbles with no spaces or punctuation. "
        f"Do NOT invent or repeat sequences. Example outputs: '<EMPTY>' or 'ABCD' or '641432'."
    )
    return prompt

# ---------- extract & heuristics ----------
def extract_sequence_from_text(text: str, vocab: Vocab, max_length: int = None, allowed_values: List[str] = None) -> str:
    if text is None:
        return ""
    txt = str(text)
    if re.search(r"<\s*EMPTY\s*>", txt, flags=re.IGNORECASE):
        return "<EMPTY>"
    if re.search(r"\bEMPTY\b", txt, flags=re.IGNORECASE):
        return "<EMPTY>"

    if allowed_values:
        allowed = set(allowed_values)
    else:
        allowed = set([tk for tk in vocab.token2idx.keys() if tk not in ['<PAD>','<SOS>','<EOS>','<EMPTY>','<UNK>']])

    seq_chars = [ch for ch in txt if ch in allowed]
    if seq_chars:
        seq = ''.join(seq_chars)
    else:
        pattern = '[' + ''.join(re.escape(c) for c in allowed) + ']+' if allowed else r'.+'
        runs = re.findall(pattern, txt)
        seq = max(runs, key=len) if runs else ""

    if seq.upper() == "EMPTY":
        return "<EMPTY>"
    if max_length:
        seq = seq[:max_length]
    return seq

def is_repetitive(seq: str, min_repeats_factor: int = 6) -> bool:
    if not seq:
        return False
    if len(set(seq)) == 1 and len(seq) > 4:
        return True
    for k in range(1,7):
        if len(seq) > k * min_repeats_factor:
            sub = seq[:k]
            if sub * (len(seq)//k) == seq[:(len(seq)//k)*k]:
                return True
    return False

# ---------- Inference (STRICT: CROPS ONLY, SKIP MISSING) ----------
def infer_sequence_from_crop_strict(entry_path: str, form_id: int, entry: dict, vocab: Vocab, model, tokenizer, debug: bool=False) -> str:
    """
    Strict inference using only pre-cropped images. If crop not found:
      - if FAIL_ON_MISSING True -> raise FileNotFoundError (fail loudly)
      - else -> return "<EMPTY>" (skip missing crops)
    """
    crop_img = load_crop_image_strict(entry_path, form_id, debug=debug)
    if crop_img is None:
        msg = f"Missing crop for {entry_path} form {form_id} in {CROP_ROOT}/{entry_path[:3]}"
        if FAIL_ON_MISSING:
            raise FileNotFoundError(msg)
        else:
            if debug:
                print(f"[W] {msg} -> mapping to '<EMPTY>' (FAIL_ON_MISSING=False).")
            return "<EMPTY>"

    # prompt values are per-entry (question type)
    prompt_values = entry.get("values", ['A','B','C','D'])
    prompt_values = [str(v) for v in prompt_values]
    if "<EMPTY>" not in prompt_values:
        prompt_values.append("<EMPTY>")

    answers = entry.get("answer", [])
    expected_len = None
    if isinstance(answers, list) and form_id < len(answers) and answers[form_id]:
        expected_len = len(answers[form_id][0])
    elif isinstance(answers, list) and len(answers) > 0 and answers[0]:
        expected_len = len(answers[0][0])

    prompt = build_sequence_prompt_for_crop(prompt_values, expected_len)
    content = [crop_img, prompt]
    msgs = [{"role": "user", "content": content}]

    if expected_len:
        max_new = max(8, min(64, expected_len * 2))
    else:
        max_new = 32

    try:
        resp = model.chat(
            msgs=msgs,
            tokenizer=tokenizer,
            stream=False,
            sampling=False,
            temperature=0.0,
            max_new_tokens=max_new
        )
    except Exception as e:
        if debug:
            print(f"[E] model.chat failed for {entry_path}, form {form_id}: {e}")
        return "<EMPTY>"

    resp_text = str(resp[0]) if isinstance(resp, (list,tuple)) else str(resp)
    seq = extract_sequence_from_text(resp_text, vocab, max_length=expected_len, allowed_values=prompt_values)

    if seq == "":
        seq = "<EMPTY>"

    if is_repetitive(seq) and seq != "<EMPTY>":
        if debug:
            print(f"[W] Detected repetitive output for {entry_path} form {form_id}; mapping to <EMPTY>")
        seq = "<EMPTY>"

    return seq

# ---------- Metrics, evaluation & visualization ----------
def normalize_empty_token(s: str) -> str:
    if s is None or s == "":
        return "<EMPTY>"
    return s

def compute_cer(pred: str, gold: str) -> float:
    pred_n = normalize_empty_token(pred)
    gold_n = normalize_empty_token(gold)
    if gold_n is None or len(gold_n) == 0:
        return float(len(pred_n) > 0)
    return editdistance.eval(pred_n, gold_n) / len(gold_n)

def token_level_metrics_from_pairs(pairs: List[Tuple[str,str]], pad_token: str = '<PAD>'):
    def to_token_list(s: str):
        if s == "<EMPTY>":
            return ['\u0001']
        return list(s)

    y_true = []
    y_pred = []
    classes = set()
    for gold, pred in pairs:
        if gold is None or gold == "":
            gold = "<EMPTY>"
        if pred is None or pred == "":
            pred = "<EMPTY>"
        gold_tokens = to_token_list(gold)
        pred_tokens = to_token_list(pred)
        maxl = max(len(gold_tokens), len(pred_tokens))
        for i in range(maxl):
            gt_tok = gold_tokens[i] if i < len(gold_tokens) else pad_token
            pr_tok = pred_tokens[i] if i < len(pred_tokens) else pad_token
            y_true.append(gt_tok)
            y_pred.append(pr_tok)
            classes.add(gt_tok); classes.add(pr_tok)

    labels = sorted(list(classes))
    total_positions = len(y_true)
    correct = sum(1 for a,b in zip(y_true,y_pred) if a==b)
    token_acc = correct / total_positions if total_positions>0 else 0.0

    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, average='micro', zero_division=0)
    per_label = {}
    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)
    for lbl, pp, rr, ff, ss in zip(labels, p, r, f, s):
        per_label[lbl] = {"precision": float(pp), "recall": float(rr), "f1": float(ff), "support": int(ss)}

    return {
        'token_accuracy': float(token_acc),
        'token_micro_precision': float(precision),
        'token_micro_recall': float(recall),
        'token_micro_f1': float(f1),
        'per_label': per_label,
        'labels': labels,
        'total_positions': total_positions,
        'correct_positions': correct
    }

def evaluate_crops_only(entries: List[dict], vocab: Vocab, model, tokenizer, save: bool=True, limit: int=None, debug: bool=False):
    details = []
    total, exact = 0, 0
    cer_sum = 0.0
    skipped = 0

    for ei, entry in enumerate(tqdm(entries, desc='Entries')):
        img_path = entry['path']
        nforms = entry.get('nforms', len(entry.get('answer_area', [])))
        try:
            nforms = int(nforms)
        except Exception:
            nforms = len(entry.get('answer_area', []))

        for form_id in range(nforms):
            total += 1
            gt = get_gt_for_form(entry, form_id)

            try:
                pred = infer_sequence_from_crop_strict(img_path, form_id, entry, vocab, model, tokenizer, debug=debug)
            except FileNotFoundError:
                if debug:
                    print(f"[W] Skipping missing crop: {img_path} form {form_id}")
                skipped += 1
                continue

            gt_norm = normalize_empty_token(gt)
            pred_norm = normalize_empty_token(pred)

            ok = (gt_norm == pred_norm)
            exact += int(ok)

            cer = compute_cer(pred_norm, gt_norm)
            cer_sum += cer

            details.append({
                "image": img_path, "form_id": form_id, "gt": gt_norm, "pred": pred_norm, "ok": ok,
                "cer": cer
            })

            if limit and total >= limit:
                break
        if limit and total >= limit:
            break

    processed = total - skipped
    acc = exact / processed if processed > 0 else 0.0
    avg_cer = cer_sum / processed if processed > 0 else 0.0

    pairs = [(d['gt'] or "", d['pred'] or "") for d in details]
    token_metrics = token_level_metrics_from_pairs(pairs)

    refs = [[list(g)] if g is not None and len(g)>0 else [['']] for g,_ in pairs]
    hyps = [list(p) if p is not None and len(p)>0 else [''] for _,p in pairs]
    refs_fixed = []
    for r in refs:
        if r == [['']] :
            refs_fixed.append([[]])
        else:
            refs_fixed.append(r)
    hyps_fixed = [h if h!=[''] else [] for h in hyps]

    if len(hyps_fixed) > 0 and len(refs_fixed) > 0:
        smoothie = SmoothingFunction().method4
        bleu1 = corpus_bleu(refs_fixed, hyps_fixed, weights=(1,0,0,0), smoothing_function=smoothie)
        bleu2 = corpus_bleu(refs_fixed, hyps_fixed, weights=(0.5,0.5,0,0), smoothing_function=smoothie)
        bleu3 = corpus_bleu(refs_fixed, hyps_fixed, weights=(1/3,1/3,1/3,0), smoothing_function=smoothie)
    else:
        bleu1 = bleu2 = bleu3 = 0.0

    out = {
        "total_attempted": total,
        "total_processed": processed,
        "skipped": skipped,
        "exact_matches": exact,
        "exact_match_accuracy": acc,
        "avg_cer": avg_cer,
        "token_metrics": token_metrics,
        "bleu1": float(bleu1),
        "bleu2": float(bleu2),
        "bleu3": float(bleu3),
        "details": details
    }

    if save:
        out_path = os.path.join(OUTPUT_DIR, "minicpm_eval_crops_strict_with_validator.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(out, f, indent=2, ensure_ascii=False)
        print(f"[i] Saved evaluation to {out_path}")

        ds_path = os.path.join(OUTPUT_DIR, "minicpm_eval_details_crops_strict_with_validator.tsv")
        with open(ds_path, 'w', encoding='utf-8') as f:
            f.write("image\tform_id\tgt\tpred\tok\tcer\n")
            for d in details:
                f.write(f"{d['image']}\t{d['form_id']}\t{d['gt']}\t{d['pred']}\t{int(d['ok'])}\t{d['cer']}\n")
        print(f"[i] Saved details to {ds_path}")

    return out

def visualize_metrics(eval_out: dict, save_dir: str):
    os.makedirs(save_dir, exist_ok=True)
    metrics = {
        'Exact-match acc': eval_out['exact_match_accuracy'],
        'Token-acc': eval_out['token_metrics']['token_accuracy'],
        'Token-F1 (micro)': eval_out['token_metrics']['token_micro_f1'],
        'BLEU-1': eval_out['bleu1'],
        'BLEU-2': eval_out['bleu2'],
        'BLEU-3': eval_out['bleu3'],
    }
    names = list(metrics.keys())
    vals = [metrics[n] for n in names]

    plt.figure(figsize=(10,5))
    plt.bar(names, vals)
    plt.ylabel('Score')
    plt.title('Evaluation summary (crops strict + validator)')
    plt.xticks(rotation=20)
    plt.tight_layout()
    out_path = os.path.join(save_dir, 'eval_summary_bar_crops_strict_validator.png')
    plt.savefig(out_path)
    plt.close()
    print(f"[i] Saved summary bar to {out_path}")

    cer_vals = [d['cer'] for d in eval_out['details']]
    plt.figure(figsize=(8,5))
    plt.hist(cer_vals, bins=30)
    plt.xlabel('CER')
    plt.ylabel('Count')
    plt.title('CER distribution (crops strict)')
    plt.tight_layout()
    out_path2 = os.path.join(save_dir, 'cer_histogram_crops_strict_validator.png')
    plt.savefig(out_path2)
    plt.close()
    print(f"[i] Saved CER histogram to {out_path2}")

    lens = [len(d['gt']) if d['gt'] else 0 for d in eval_out['details']]
    plt.figure(figsize=(8,5))
    plt.scatter(lens, cer_vals, alpha=0.6)
    plt.xlabel('GT sequence length')
    plt.ylabel('CER')
    plt.title('CER vs GT length (crops strict)')
    plt.tight_layout()
    out_path3 = os.path.join(save_dir, 'cer_vs_len_scatter_crops_strict_validator.png')
    plt.savefig(out_path3)
    plt.close()
    print(f"[i] Saved CER vs length scatter to {out_path3}")

# ---------- MAIN ----------
if _name_ == "_main_":
    gc.collect()
    entries = load_json_entries(JSON_PATH)
    lookup = build_lookup_map(entries)

    # 1) Validate crops before loading model
    print("[i] Validating expected crop files (pre-check)...")
    missing = validate_and_report(entries, save_csv=True)
    if len(missing) > 0:
        print(f"[W] Found {len(missing)} missing crop files (see {MISSING_CSV_PATH})")
        if FAIL_ON_MISSING:
            # fail loudly so user can fix inputs before heavy model load
            raise FileNotFoundError(f"Missing crop files detected. See {MISSING_CSV_PATH} for details. Aborting due to FAIL_ON_MISSING=True.")
        else:
            print("[W] Continuing despite missing crops (FAIL_ON_MISSING=False). Missing crops will be skipped during inference.")

    # 2) Load model + vocab after pre-check
    model, tokenizer = load_model_and_tokenizer(MODEL_ID, DTYPE, DEVICE)
    vocab = load_or_build_vocab(VOCAB_PATH, JSON_PATH)

    # Quick example tests - updated to use your available crops
    EXAMPLE_PATHS = [
        ("001/1234567891019.png", 0),
        ("005/1234567891019.png", 0),
        ("005/1234567891019.png", 1),
        ("048/1234567891071.png", 0),
        ("007/1234567891019.png", 0),
    ]

    if len(EXAMPLE_PATHS) > 0:
        print(f"[i] Running quick test on {len(EXAMPLE_PATHS)} examples (CROPS ONLY)...")
        for img_path, form_id in EXAMPLE_PATHS:
            entry = lookup.get(img_path)
            if entry is None:
                print(f"[W] Example {img_path} not found in lookup; skipping")
                continue
            try:
                pred = infer_sequence_from_crop_strict(img_path, form_id, entry, vocab, model, tokenizer, debug=True)
            except FileNotFoundError as e:
                print(f"[E] {e}")
                pred = "<EMPTY>"
            gt = get_gt_for_form(entry, form_id)
            print(f"[example_test] image={img_path} form={form_id} GT={normalize_empty_token(gt)} PRED={normalize_empty_token(pred)}")
    else:
        print("[i] No quick examples provided. To run quick tests, edit EXAMPLE_PATHS list in the script.")

    # # 3) Run full batch evaluation
    print(f"[i] Running batch evaluation on {len(entries)} entries (CROPS ONLY)...")
    results = evaluate_crops_only(entries, vocab, model, tokenizer, save=True, limit=None, debug=False)

    visualize_metrics(results, OUTPUT_DIR)

    # Print summary
    print('[i] SUMMARY (crops strict + validator):')
    print(f" Total forms attempted: {results['total_attempted']}")
    print(f" Total forms processed: {results['total_processed']}")
    print(f" Forms skipped (missing crops): {results['skipped']}")
    print(f" Exact-match accuracy: {results['exact_match_accuracy']:.4f}")
    print(f" Avg CER: {results['avg_cer']:.4f}")
    print(f" Token accuracy: {results['token_metrics']['token_accuracy']:.4f}")
    print(f" Token micro-F1: {results['token_metrics']['token_micro_f1']:.4f}")
    print(f" BLEU-1: {results['bleu1']:.4f}, BLEU-2: {results['bleu2']:.4f}, BLEU-3: {results['bleu3']:.4f}")

    printed = 0
    for d in results['details']:
        if printed >= 4:
            break
        if d['gt'] is None:
            continue
        print(f"[example] image={d['image']} form={d['form_id']} GT={d['gt']} PRED={d['pred']} CER={d['cer']:.3f}")
        printed += 1

    print('[i] Done. All outputs and plots are in OUTPUT_DIR:', OUTPUT_DIR)