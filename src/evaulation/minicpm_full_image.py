# -*- coding: utf-8 -*-
"""MiniCPM-V-4_5_NoSegmentation.ipynb adlı dosyanın kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iToIu_qWML05J3ZGSnu_AlgY7t7LjpXd
"""

# minimal edits to run on full image (no cropping / no bounding-box prompt)
import os
import json
import re
import gc
import torch
import editdistance
import pickle
from PIL import Image, ImageEnhance
from typing import List, Tuple
from transformers import AutoModel, AutoTokenizer
import cv2
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter, defaultdict

from sklearn.metrics import precision_recall_fscore_support
import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

from google.colab import drive
drive.mount('/content/drive')

# ---------- CONFIG ----------
ROOT = "/content/drive/MyDrive/Aligned_Sheets"
JSON_PATH = os.path.join(ROOT, "dataset_updated.json")
OUTPUT_DIR = os.path.join(ROOT, "minicpm_inference_outputs_fullsheet_v2_batch_no_crop")
os.makedirs(OUTPUT_DIR, exist_ok=True)

VOCAB_PATH = os.path.join(ROOT, "vocab.pkl")

MODEL_ID = "openbmb/MiniCPM-V-4_5-AWQ"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float32

print(f"[i] DEVICE={DEVICE}, DTYPE={DTYPE}, MODEL_ID={MODEL_ID}")

# ---------- Vocab ----------
class Vocab:
    def __init__(self):
        self.token2idx = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3, "<EMPTY>": 4}
        self.idx2token = {idx: tk for tk, idx in self.token2idx.items()}
        self.next_idx = 5

    def add_sentence(self, sentence):
        if sentence is None:
            return
        if sentence == "":
            return
        for ch in sentence:
            if ch not in self.token2idx:
                self.token2idx[ch] = self.next_idx
                self.idx2token[self.next_idx] = ch
                self.next_idx += 1

    def __len__(self):
        return len(self.token2idx)

# ---------- JSON helpers ----------
def load_json_entries(json_path: str, excluded_prefixes: set = None) -> List[dict]:
    excluded = excluded_prefixes or {'002','004','009','018','020','022','024','025','027','039','040','043'}
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    out = [e for e in data if e.get('alignment','horizontal') == 'horizontal' and e.get('path','')[:3] not in excluded]
    print(f"[i] Loaded {len(out)} filtered entries from {json_path}")
    return out


def build_lookup_map(entries: List[dict]) -> dict:
    return {e['path']: e for e in entries}


def get_gt_for_form(entry: dict, form_id: int):
    answers = entry.get('answer', [])
    if isinstance(answers, list) and form_id < len(answers) and answers[form_id]:
        return answers[form_id][0]
    if isinstance(answers, list) and len(answers) > 0 and answers[0]:
        return answers[0][0]
    return ""

# ---------- Image helpers ----------
FULL_IMAGES_ROOT = os.path.join(ROOT, "full_images")  # adjust if needed

def load_full_image(entry_path: str) -> Image.Image:
    name = os.path.basename(entry_path)
    cand1 = os.path.join(FULL_IMAGES_ROOT, name)
    cand2 = os.path.join(ROOT, entry_path)
    if os.path.exists(cand1):
        full_path = cand1
    elif os.path.exists(cand2):
        full_path = cand2
    else:
        raise FileNotFoundError(f"Full image not found for {entry_path} (tried {cand1} and {cand2})")
    img = Image.open(full_path).convert("RGB")
    img = ImageEnhance.Sharpness(img).enhance(1.2)
    img = ImageEnhance.Contrast(img).enhance(1.1)
    return img

# (keep crop_in_memory in file if you want it for other experiments, but it will NOT be used here)

def crop_in_memory(full_img: Image.Image, bbox: List[int], out_size: Tuple[int,int]=(512,512), pad:int=8) -> Image.Image:
    if not bbox or len(bbox) != 4:
        return None
    x1,y1,x2,y2 = bbox
    x1 = max(0, x1 - pad); y1 = max(0, y1 - pad)
    x2 = min(full_img.width, x2 + pad); y2 = min(full_img.height, y2 + pad)
    crop = full_img.crop((x1,y1,x2,y2)).resize(out_size, Image.LANCZOS)
    gray = np.array(crop.convert('L'))
    try:
        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY_INV, 15, 4)
        crop = Image.fromarray(thresh).convert("RGB")
    except Exception:
        pass
    return crop

# ---------- Model load ----------
print("[i] Loading MiniCPM model & tokenizer...")
try:
    model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True, torch_dtype=DTYPE).eval().to(DEVICE)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    print(f"[i] Loaded model {MODEL_ID} on {DEVICE}, dtype={DTYPE}")
except Exception as e:
    print(f"[W] Primary model load failed: {e}. Trying fallback model id.")
    MODEL_ID_FALLBACK = "openbmb/MiniCPM-V-4_5"
    model = AutoModel.from_pretrained(MODEL_ID_FALLBACK, trust_remote_code=True, torch_dtype=DTYPE).eval().to(DEVICE)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID_FALLBACK, trust_remote_code=True)
    print(f"[i] Loaded fallback model {MODEL_ID_FALLBACK} on {DEVICE}")

# ---------- Vocab load/build ----------
if os.path.exists(VOCAB_PATH):
    with open(VOCAB_PATH, "rb") as f:
        vocab = pickle.load(f)
    print(f"[i] Loaded vocab from {VOCAB_PATH}, size={len(vocab)}")
else:
    print("[i] Building dataset-wide vocab from dataset answers...")
    entries_tmp = load_json_entries(JSON_PATH)
    vocab = Vocab()
    for e in entries_tmp:
        for ans_list in e.get("answer", []):
            if ans_list and len(ans_list) > 0:
                vocab.add_sentence(ans_list[0])
    with open(VOCAB_PATH, "wb") as f:
        pickle.dump(vocab, f)
    print(f"[i] Built and saved vocab to {VOCAB_PATH}, size={len(vocab)}")

# ---------- Prompt (full-image, no bbox) ----------
def build_sequence_prompt_full_image(valid_values: List[str], expected_len: int = None) -> str:
    """
    Prompt that instructs the model to read the ENTIRE image.
    No coordinates, no bounding box language.
    """
    vals = ' '.join(valid_values)
    exp_text = f"Expected length = {expected_len}. Return EXACTLY this many symbols if possible. " if expected_len else ""
    prompt = (
        f"You are given an exam-sheet image (the WHOLE PAGE). "
        f"The valid symbols that can appear are exactly: {vals}. "
        f"{exp_text}"
        f"Read the image left-to-right, top-to-bottom (row-major) across the entire page. "
        f"If no marked bubbles are visible for the requested field, reply with the single token: <EMPTY> (including angle brackets). "
        f"Return ONLY the concatenated sequence of symbols for marked bubbles with no spaces or punctuation. "
        f"Do NOT invent or repeat sequences. Example outputs: '<EMPTY>' or 'ABCD' or '641432'."
    )
    return prompt

# ---------- extract & heuristics (kept unchanged) ----------
def extract_sequence_from_text(text: str, vocab: Vocab, max_length: int = None, allowed_values: List[str] = None) -> str:
    if text is None:
        return ""
    txt = str(text)
    if re.search(r"<\s*EMPTY\s*>", txt, flags=re.IGNORECASE):
        return "<EMPTY>"
    if re.search(r"\bEMPTY\b", txt, flags=re.IGNORECASE):
        return "<EMPTY>"

    if allowed_values:
        allowed = set(allowed_values)
    else:
        allowed = set([tk for tk in vocab.token2idx.keys() if tk not in ['<PAD>','<SOS>','<EOS>','<EMPTY>','<UNK>']])

    seq_chars = [ch for ch in txt if ch in allowed]
    if seq_chars:
        seq = ''.join(seq_chars)
    else:
        pattern = '[' + ''.join(re.escape(c) for c in allowed) + ']+' if allowed else r'.+'
        runs = re.findall(pattern, txt)
        seq = max(runs, key=len) if runs else ""

    if seq.upper() == "EMPTY":
        return "<EMPTY>"
    if max_length:
        seq = seq[:max_length]
    return seq

def is_repetitive(seq: str, min_repeats_factor: int = 6) -> bool:
    if not seq:
        return False
    if len(set(seq)) == 1 and len(seq) > 4:
        return True
    for k in range(1,7):
        if len(seq) > k * min_repeats_factor:
            sub = seq[:k]
            if sub * (len(seq)//k) == seq[:(len(seq)//k)*k]:
                return True
    return False

# ---------- Inference (NO CROP, NO COORDS) ----------
def infer_sequence_full(image_path: str, bbox: List[int], entry: dict, form_id: int, vocab: Vocab, debug: bool=False) -> str:
    """
    NOTE: This version ignores bbox and runs on the entire full image.
    """
    try:
        full_img = load_full_image(image_path)
    except Exception as e:
        if debug:
            print(f"[E] load_full_image failed for {image_path}: {e}")
        return ""

    prompt_values = entry.get("values", ['A','B','C','D'])
    prompt_values = [str(v) for v in prompt_values]
    if "<EMPTY>" not in prompt_values:
        prompt_values.append("<EMPTY>")

    answers = entry.get("answer", [])
    expected_len = None
    if isinstance(answers, list) and form_id < len(answers) and answers[form_id]:
        expected_len = len(answers[form_id][0])
    elif isinstance(answers, list) and len(answers) > 0 and answers[0]:
        expected_len = len(answers[0][0])

    # Build a prompt that describes the whole page (no coords)
    prompt = build_sequence_prompt_full_image(prompt_values, expected_len)

    # Always send entire image + prompt
    content = [full_img, prompt]
    msgs = [{"role": "user", "content": content}]

    if expected_len:
        max_new = max(8, min(64, expected_len * 2))
    else:
        max_new = 32

    try:
        resp = model.chat(
            msgs=msgs,
            tokenizer=tokenizer,
            stream=False,
            sampling=False,
            temperature=0.0,
            max_new_tokens=max_new
        )
    except Exception as e:
        if debug:
            print(f"[E] model.chat failed for {image_path}, form {form_id}: {e}")
        return ""

    resp_text = str(resp[0]) if isinstance(resp, (list,tuple)) else str(resp)
    seq = extract_sequence_from_text(resp_text, vocab, max_length=expected_len, allowed_values=prompt_values)

    if seq == "":
        seq = "<EMPTY>"

    if is_repetitive(seq) and seq != "<EMPTY>":
        if debug:
            print(f"[W] Detected repetitive output for {image_path} form {form_id}; mapping to <EMPTY>")
        seq = "<EMPTY>"

    return seq

# ---------- Metrics, evaluation & visualization  ----------
def normalize_empty_token(s: str) -> str:
    if s is None or s == "":
        return "<EMPTY>"
    return s


def compute_cer(pred: str, gold: str) -> float:
    pred_n = normalize_empty_token(pred)
    gold_n = normalize_empty_token(gold)
    if gold_n is None or len(gold_n) == 0:
        return float(len(pred_n) > 0)
    return editdistance.eval(pred_n, gold_n) / len(gold_n)


def token_level_metrics_from_pairs(pairs: List[Tuple[str,str]], pad_token: str = '<PAD>'):
    def to_token_list(s: str):
        if s == "<EMPTY>":
            return ['\u0001']
        return list(s)

    y_true = []
    y_pred = []
    classes = set()
    for gold, pred in pairs:
        if gold is None or gold == "":
            gold = "<EMPTY>"
        if pred is None or pred == "":
            pred = "<EMPTY>"
        gold_tokens = to_token_list(gold)
        pred_tokens = to_token_list(pred)
        maxl = max(len(gold_tokens), len(pred_tokens))
        for i in range(maxl):
            gt_tok = gold_tokens[i] if i < len(gold_tokens) else pad_token
            pr_tok = pred_tokens[i] if i < len(pred_tokens) else pad_token
            y_true.append(gt_tok)
            y_pred.append(pr_tok)
            classes.add(gt_tok); classes.add(pr_tok)

    labels = sorted(list(classes))
    total_positions = len(y_true)
    correct = sum(1 for a,b in zip(y_true,y_pred) if a==b)
    token_acc = correct / total_positions if total_positions>0 else 0.0

    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, average='micro', zero_division=0)
    per_label = {}
    p,r,f,s = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)
    for lbl, pp, rr, ff, ss in zip(labels, p, r, f, s):
        per_label[lbl] = {"precision": float(pp), "recall": float(rr), "f1": float(ff), "support": int(ss)}

    return {
        'token_accuracy': float(token_acc),
        'token_micro_precision': float(precision),
        'token_micro_recall': float(recall),
        'token_micro_f1': float(f1),
        'per_label': per_label,
        'labels': labels,
        'total_positions': total_positions,
        'correct_positions': correct
    }

def evaluate_full_images(entries: List[dict], vocab: Vocab, save: bool=True, limit: int=None):
    details = []
    total, exact = 0, 0
    cer_sum = 0.0

    for ei, entry in enumerate(tqdm(entries, desc='Entries')):
        img_path = entry['path']
        nforms = entry.get('nforms', len(entry.get('answer_area', [])))
        areas = entry.get('answer_area', [])
        if isinstance(areas, list) and len(areas) == 4 and isinstance(areas[0], int):
            areas = [areas]

        for form_id in range(nforms):
            total += 1
            # we run on full image
            bbox = None
            gt = get_gt_for_form(entry, form_id)
            pred = infer_sequence_full(img_path, bbox, entry, form_id, vocab)


            gt_norm = normalize_empty_token(gt)
            pred_norm = normalize_empty_token(pred)

            ok = (gt_norm == pred_norm)
            exact += int(ok)

            cer = compute_cer(pred_norm, gt_norm)
            cer_sum += cer

            details.append({
                "image": img_path, "form_id": form_id, "gt": gt_norm, "pred": pred_norm, "ok": ok,
                "cer": cer
            })

            if limit and total >= limit:
                break
        if limit and total >= limit:
            break

    acc = exact / total if total > 0 else 0.0
    avg_cer = cer_sum / total if total > 0 else 0.0

    pairs = [(d['gt'] or "", d['pred'] or "") for d in details]
    token_metrics = token_level_metrics_from_pairs(pairs)

    refs = [[list(g)] if g is not None and len(g)>0 else [['']] for g,_ in pairs]
    hyps = [list(p) if p is not None and len(p)>0 else [''] for _,p in pairs]
    refs_fixed = []
    for r in refs:
        if r == [['']] :
            refs_fixed.append([[]])
        else:
            refs_fixed.append(r)
    hyps_fixed = [h if h!=[''] else [] for h in hyps]

    smoothie = SmoothingFunction().method4
    bleu1 = corpus_bleu(refs_fixed, hyps_fixed, weights=(1,0,0,0), smoothing_function=smoothie)
    bleu2 = corpus_bleu(refs_fixed, hyps_fixed, weights=(0.5,0.5,0,0), smoothing_function=smoothie)
    bleu3 = corpus_bleu(refs_fixed, hyps_fixed, weights=(1/3,1/3,1/3,0), smoothing_function=smoothie)

    out = {
        "total": total,
        "exact_matches": exact,
        "exact_match_accuracy": acc,
        "avg_cer": avg_cer,
        "token_metrics": token_metrics,
        "bleu1": float(bleu1),
        "bleu2": float(bleu2),
        "bleu3": float(bleu3),
        "details": details
    }

    if save:
        out_path = os.path.join(OUTPUT_DIR, "minicpm_eval_fullimages_v2_batch_no_crop.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(out, f, indent=2, ensure_ascii=False)
        print(f"[i] Saved evaluation to {out_path}")

        ds_path = os.path.join(OUTPUT_DIR, "minicpm_eval_details_no_crop.tsv")
        with open(ds_path, 'w', encoding='utf-8') as f:
            f.write("image\tform_id\tgt\tpred\tok\tcer\n")
            for d in details:
                f.write(f"{d['image']}\t{d['form_id']}\t{d['gt']}\t{d['pred']}\t{int(d['ok'])}\t{d['cer']}\n")
        print(f"[i] Saved details to {ds_path}")

    return out

def visualize_metrics(eval_out: dict, save_dir: str):
    os.makedirs(save_dir, exist_ok=True)
    metrics = {
        'Exact-match acc': eval_out['exact_match_accuracy'],
        'Token-acc': eval_out['token_metrics']['token_accuracy'],
        'Token-F1 (micro)': eval_out['token_metrics']['token_micro_f1'],
        'BLEU-1': eval_out['bleu1'],
        'BLEU-2': eval_out['bleu2'],
        'BLEU-3': eval_out['bleu3'],
    }
    names = list(metrics.keys())
    vals = [metrics[n] for n in names]

    plt.figure(figsize=(10,5))
    plt.bar(names, vals)
    plt.ylabel('Score')
    plt.title('Evaluation summary (no-crop)')
    plt.xticks(rotation=20)
    plt.tight_layout()
    out_path = os.path.join(save_dir, 'eval_summary_bar_no_crop.png')
    plt.savefig(out_path)
    plt.close()
    print(f"[i] Saved summary bar to {out_path}")

    cer_vals = [d['cer'] for d in eval_out['details']]
    plt.figure(figsize=(8,5))
    plt.hist(cer_vals, bins=30)
    plt.xlabel('CER')
    plt.ylabel('Count')
    plt.title('CER distribution (no-crop)')
    plt.tight_layout()
    out_path2 = os.path.join(save_dir, 'cer_histogram_no_crop.png')
    plt.savefig(out_path2)
    plt.close()
    print(f"[i] Saved CER histogram to {out_path2}")

    lens = [len(d['gt']) if d['gt'] else 0 for d in eval_out['details']]
    plt.figure(figsize=(8,5))
    plt.scatter(lens, cer_vals, alpha=0.6)
    plt.xlabel('GT sequence length')
    plt.ylabel('CER')
    plt.title('CER vs GT length (no-crop)')
    plt.tight_layout()
    out_path3 = os.path.join(save_dir, 'cer_vs_len_scatter_no_crop.png')
    plt.savefig(out_path3)
    plt.close()
    print(f"[i] Saved CER vs length scatter to {out_path3}")

# ---------- MAIN ----------
if __name__ == "__main__":
    gc.collect()
    entries = load_json_entries(JSON_PATH)
    lookup = build_lookup_map(entries)

    EXAMPLE_PATHS = [
        ("001/1234567891019.png", 0),
        ("048/1234567891071.png", 1),
        ("048/1234567891071.png", 2),
        ("005/1234567891699.png", 0),
        ("005/1234567892252.png", 2),
    ]

    if len(EXAMPLE_PATHS) > 0:
        print(f"[i] Running quick test on {len(EXAMPLE_PATHS)} examples (NO CROP)...")
        for img_path, form_id in EXAMPLE_PATHS:
            entry = lookup.get(img_path)
            if entry is None:
                print(f"[W] Example {img_path} not found in lookup; skipping")
                continue
            # we intentionally pass bbox=None to force full-image behavior
            pred = infer_sequence_full(img_path, None, entry, form_id, vocab, debug=True)
            gt = get_gt_for_form(entry, form_id)
            print(f"[example_test] image={img_path} form={form_id} GT={normalize_empty_token(gt)} PRED={normalize_empty_token(pred)}")
    else:
        print("[i] No quick examples provided. To run quick tests, edit EXAMPLE_PATHS list in the script.")

    print(f"[i] Running batch evaluation on {len(entries)} entries (NO CROP)...")
    results = evaluate_full_images(entries, vocab, save=True, limit=None)

    visualize_metrics(results, OUTPUT_DIR)

    # Print summary
    print('[i] SUMMARY (no-crop):')
    print(f" Total forms evaluated: {results['total']}")
    print(f" Exact-match accuracy: {results['exact_match_accuracy']:.4f}")
    print(f" Avg CER: {results['avg_cer']:.4f}")
    print(f" Token accuracy: {results['token_metrics']['token_accuracy']:.4f}")
    print(f" Token micro-F1: {results['token_metrics']['token_micro_f1']:.4f}")
    print(f" BLEU-1: {results['bleu1']:.4f}, BLEU-2: {results['bleu2']:.4f}, BLEU-3: {results['bleu3']:.4f}")

    printed = 0
    for d in results['details']:
        if printed >= 4:
            break
        if d['gt'] is None:
            continue
        print(f"[example] image={d['image']} form={d['form_id']} GT={d['gt']} PRED={d['pred']} CER={d['cer']:.3f}")
        printed += 1

    print('[i] Done. All outputs and plots are in OUTPUT_DIR:', OUTPUT_DIR)