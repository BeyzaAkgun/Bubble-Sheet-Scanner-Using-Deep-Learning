# -*- coding: utf-8 -*-
"""nanonets_full_paper.ipynb adlı dosyanın kopyası

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19RyMBdTujrmpoPMDhxzQX6twO1BSMnW-
"""

!pip -q install "transformers>=4.42.0" accelerate pillow editdistance opencv-python-headless --upgrade

import os, json, re, gc
import torch, pickle
from typing import List, Tuple, Optional, Dict, Any
from PIL import Image, ImageEnhance, ImageOps
import numpy as np
import cv2
import editdistance
from transformers import AutoModelForCausalLM, AutoProcessor

# Mount Drive (Colab)
from google.colab import drive
drive.mount('/content/drive')

# ---------- CONFIG ----------
def pick_dtype():
    if torch.cuda.is_available():
        major, _ = torch.cuda.get_device_capability()
        # T4 (cc 7.x) => fp16, Ampere+ (>=8.x) => bf16
        return torch.float16 if major < 8 else torch.bfloat16
    return torch.float32

DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE    = pick_dtype()
MODEL_ID = "nanonets/Nanonets-OCR-s"

print(f"[i] DEVICE={DEVICE}, cap={torch.cuda.get_device_capability(0) if torch.cuda.is_available() else 'CPU'}, "
      f"DTYPE={DTYPE}, MODEL_ID={MODEL_ID}")

# Project paths
ROOT             = "/content/drive/MyDrive/Aligned_Sheets1"
JSON_PATH        = f"{ROOT}/dataset_updated.json"
OUTPUT_DIR       = f"{ROOT}/nanonets_inference_outputs_fullsheet_total"
VOCAB_PATH       = f"{ROOT}/vocab.pkl"
FULL_IMAGES_ROOT = ROOT  # change if full images live elsewhere

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Optional: use all CPU threads when no GPU
if DEVICE == "cpu":
    torch.set_num_threads(max(1, os.cpu_count() or 1))

# Vocab

SPECIAL = {"<PAD>", "<SOS>", "<EOS>", "<UNK>", "<EMPTY>"}

class Vocab:
    def __init__(self, base_tokens: Optional[List[str]] = None):
        # Special tokens
        self.token2idx = {"<PAD>": 0, "<SOS>": 1, "<EOS>": 2, "<UNK>": 3, "<EMPTY>": 4}
        self.idx2token = {v: k for k, v in self.token2idx.items()}
        self.next_idx = 5

        # Optionally seed with allowed symbols (e.g., digits or A–E)
        if base_tokens:
            self.add_tokens(base_tokens)

    def add_tokens(self, tokens: List[str]):
        for tk in tokens:
            if tk not in self.token2idx:
                self.token2idx[tk] = self.next_idx
                self.idx2token[self.next_idx] = tk
                self.next_idx += 1

    def add_sentence(self, sentence: str):
        if sentence == "":
            sentence = "<EMPTY>"
        for ch in sentence:
            if ch not in self.token2idx:
                self.token2idx[ch] = self.next_idx
                self.idx2token[self.next_idx] = ch
                self.next_idx += 1

    def allowed_set(self):
        # All tokens except special ones
        return {tk for tk in self.token2idx.keys() if tk not in SPECIAL}

    def __len__(self):
        return len(self.token2idx)

    # I/O
    def save(self, path: str):
        with open(path, "wb") as f:
            pickle.dump(self, f)

    @staticmethod
    def load(path: str) -> "Vocab":
        with open(path, "rb") as f:
            return pickle.load(f)


def build_vocab_from_dataset(entries: List[Dict[str, Any]],
                             base_tokens: Optional[List[str]] = None) -> Vocab:
    """
    entries: dataset JSON (list of dicts). Looks for entry['answer'] which may be list of [seq] items.
    base_tokens: seed allowed tokens explicitly, e.g. list('0123456789') or list('ABCDE')
    """
    vocab = Vocab(base_tokens=base_tokens)
    for e in entries:
        # answers could be like: [["641432160822"], ["..."], ...] or [[]]
        ans_list = e.get("answer", [])
        for item in ans_list:
            if item and len(item) > 0 and isinstance(item[0], str):
                vocab.add_sentence(item[0])
    return vocab


# ---- Example usage in your pipeline ----
# 1) digits-only task
# vocab = build_vocab_from_dataset(entries, base_tokens=list("0123456789"))

# 2) multiple-choice A–E task
# vocab = build_vocab_from_dataset(entries, base_tokens=list("ABCDE"))

#JSON helpers

from typing import List, Dict, Any, Optional
import os, json

def _normalize_answer_area(areas: Any) -> List[List[int]]:
    """
    Accepts:
      - [x1,y1,x2,y2]
      - [[x1,y1,x2,y2], ...]
      - None / []
    Returns list of [x1,y1,x2,y2] (ints).
    """
    if not areas:
        return []
    # single box
    if isinstance(areas, list) and len(areas) == 4 and all(isinstance(v, (int, float)) for v in areas):
        return [[int(areas[0]), int(areas[1]), int(areas[2]), int(areas[3])]]
    # list of boxes
    if isinstance(areas, list) and all(isinstance(b, list) and len(b) == 4 for b in areas):
        return [[int(b[0]), int(b[1]), int(b[2]), int(b[3])] for b in areas]
    return []

def _coerce_path(p: Optional[str]) -> Optional[str]:
    """Return as-is; downstream load_full_image resolves to FULL_IMAGES_ROOT/ROOT."""
    if not p:
        return None
    return p

def _coerce_values(vals: Any, default: str = "0123456789") -> List[str]:
    """
    values may be like ['A','B','C','D'] or 'ABCDE' or None.
    """
    if vals is None:
        return list(default)
    if isinstance(vals, str):
        return [c for c in vals]
    if isinstance(vals, (list, tuple)):
        return [str(v) for v in vals]
    return list(default)

def _extract_gt_list(ans_field: Any) -> List[str]:
    """
    Normalizes various dataset shapes to a flat list of GT strings:
      - ["641432160822", "..."]
      - [["641432160822"], []]
      - "641432160822"
    """
    out = []
    if ans_field is None:
        return out
    if isinstance(ans_field, str):
        out.append(ans_field)
        return out
    if isinstance(ans_field, list):
        for item in ans_field:
            if isinstance(item, str):
                out.append(item)
            elif isinstance(item, list) and item and isinstance(item[0], str):
                out.append(item[0])
    return out

def load_json_entries(json_path: str,
                      excluded_prefixes: Optional[set] = None,
                      filter_horizontal: bool = True) -> List[dict]:
    """
    Loads dataset, optionally filters by alignment=='horizontal' and by first 3-char prefix.
    Also canonicalizes a few fields: path, answer_area, nforms, values.
    """
    excluded = excluded_prefixes or {'002','004','009','018','020','022','024','025','027','039','040','043'}
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    out = []
    for e in data:
        if filter_horizontal and e.get('alignment', 'horizontal') != 'horizontal':
            continue

        path = _coerce_path(e.get('path') or e.get('image') or e.get('img_path'))
        if not path:
            continue

        # exclude by path prefix (first 3 chars)
        prefix3 = (path[:3] if len(path) >= 3 else path)
        if prefix3 in excluded:
            continue

        areas = _normalize_answer_area(e.get('answer_area'))
        nforms = e.get('nforms', len(areas))
        values = _coerce_values(e.get('values'), default="0123456789")

        # keep original 'answer' but add a normalized view for convenience
        gt_list = _extract_gt_list(e.get('answer'))

        out.append({
            **e,  # keep all original keys
            'path': path,
            'answer_area': areas,
            'nforms': nforms,
            'values': values,
            '_gt_list': gt_list,   # helper view (non-breaking)
        })

    print(f"[i] Loaded {len(out)} filtered entries from {json_path}")
    return out

def build_lookup_map(entries: List[dict]) -> dict:
    # path -> entry
    return {e.get('path'): e for e in entries if e.get('path')}

# ---------- Image helpers ----------
def _clamp(v, lo, hi):
    return max(lo, min(int(round(v)), hi))

def load_full_image(entry_path: str, max_side: int = 2200) -> Image.Image:
    """
    Try multiple candidate paths, open with EXIF-aware orientation,
    light enhancement, and optional downscale to keep memory reasonable.
    """
    # candidate paths
    name  = os.path.basename(entry_path or "")
    cand0 = entry_path if entry_path and os.path.exists(entry_path) else None
    cand1 = os.path.join(FULL_IMAGES_ROOT, name) if name else None
    cand2 = os.path.join(ROOT, entry_path) if entry_path else None

    full_path = None
    for c in (cand0, cand1, cand2):
        if c and os.path.exists(c):
            full_path = c
            break
    if not full_path:
        raise FileNotFoundError(f"Full image not found for {entry_path} (tried {cand0}, {cand1}, {cand2})")

    # open + EXIF transpose
    img = Image.open(full_path)
    img = ImageOps.exif_transpose(img).convert("RGB")

    # gentle enhancement
    img = ImageEnhance.Sharpness(img).enhance(1.15)
    img = ImageEnhance.Contrast(img).enhance(1.08)

    # optional downscale to limit extremely large pages (keeps aspect)
    if max_side and max(img.size) > max_side:
        w, h = img.size
        scale = max_side / float(max(w, h))
        img = img.resize((int(w * scale), int(h * scale)), Image.LANCZOS)

    return img

def crop_in_memory(
    full_img: Image.Image,
    bbox: list,
    out_size: tuple = (512, 512),
    pad: int = 8,
    apply_threshold: bool = True
) -> Image.Image:
    """
    Crop ROI with padding, resize to out_size, and (optionally) apply adaptive threshold.
    Returns an RGB PIL image ready for the VLM.
    """
    if not bbox or len(bbox) != 4:
        return None

    W, H = full_img.width, full_img.height
    x1, y1, x2, y2 = bbox
    # accept float/int bboxes
    x1 = _clamp(x1 - pad, 0, W - 1)
    y1 = _clamp(y1 - pad, 0, H - 1)
    x2 = _clamp(x2 + pad, 0, W)
    y2 = _clamp(y2 + pad, 0, H)
    if x2 <= x1 or y2 <= y1:
        return None

    crop = full_img.crop((x1, y1, x2, y2)).resize(out_size, Image.LANCZOS)

    if not apply_threshold:
        return crop

    # grayscale + adaptive threshold (if cv2 available)
    gray = np.array(crop.convert("L"))
    try:
        import cv2
        thr = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV, 15, 4
        )
        crop = Image.fromarray(thr).convert("RGB")
    except Exception:
        # fallback: mild contrast via autocontrast; stay in RGB
        crop = ImageOps.autocontrast(crop, cutoff=1)

    return crop

# Ensure right Transformers version (needs >=4.45 for Qwen2.5-VL autos)
import transformers, sys, subprocess, importlib
ver = tuple(int(x) for x in transformers.__version__.split(".")[:2])
if ver < (4, 45):
    print(f"[i] Upgrading transformers {transformers.__version__} -> >=4.45.0 ...")
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "transformers>=4.45.0", "accelerate", "--upgrade"], check=True)
    importlib.reload(transformers)

# ---------- Model load (Nanonets-OCR-s) ----------
from transformers import AutoProcessor
try:
    # Preferred generic auto class for VLMs
    from transformers import AutoModelForVision2Seq
    MODEL_CLASS = AutoModelForVision2Seq
except Exception:
    # Fallback: direct model class if AutoModelForVision2Seq isn't available
    from transformers import Qwen2_5_VLForConditionalGeneration as AutoModelForVision2Seq
    MODEL_CLASS = AutoModelForVision2Seq

print("[i] Loading Nanonets-OCR-s (Qwen2.5-VL finetune)...")
model = MODEL_CLASS.from_pretrained(
    MODEL_ID,
    dtype=DTYPE,
    device_map="auto",
    trust_remote_code=True
).eval()

processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
print(f"[i] Loaded {MODEL_ID} on {DEVICE} (dtype={DTYPE})")


@torch.inference_mode()
def nano_generate(images, prompt, max_new_tokens=256):
    if images is None:
        images = []
    if not isinstance(images, (list, tuple)):
        images = [images]

    messages = [
        {"role": "system", "content": "You are a helpful, precise document OCR & form-understanding assistant."},
        {"role": "user", "content": [{"type": "text", "text": prompt}] + [{"type": "image"} for _ in images]} ,
    ]

    chat = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=[chat], images=images, return_tensors="pt").to(model.device)

    gen_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,   # deterministic

    )

    new_tokens = gen_ids[:, inputs["input_ids"].shape[1]:]
    out = processor.batch_decode(new_tokens, skip_special_tokens=True)[0].strip()
    return out

def collect_base_tokens_from_entries(entries: List[Dict[str, Any]], default: str = "0123456789") -> List[str]:
    """
    Derives the allowed symbol set from dataset entries['values'].
    Falls back to digits if unspecified.
    """
    toks = set()
    for e in entries:
        vals = e.get("values")
        if isinstance(vals, str):
            toks.update(list(vals))
        elif isinstance(vals, (list, tuple)):
            toks.update(str(v) for v in vals)
    if not toks:
        toks.update(list(default))
    return sorted(toks)

if os.path.exists(VOCAB_PATH):
    with open(VOCAB_PATH, "rb") as f:
        vocab = pickle.load(f)
    try:
        allowed_view = sorted(vocab.allowed_set())
    except Exception:
        # older Vocab without allowed_set()
        SPECIAL = {"<PAD>", "<SOS>", "<EOS>", "<UNK>", "<EMPTY>"}
        allowed_view = sorted([k for k in getattr(vocab, "token2idx", {}).keys() if k not in SPECIAL])
    print(f"[i] Loaded vocab from {VOCAB_PATH}, size={len(vocab)}; allowed={allowed_view}")
else:
    print("[i] Building vocab from dataset…")
    entries_tmp = load_json_entries(JSON_PATH)  # uses your JSON helpers
    base_tokens = collect_base_tokens_from_entries(entries_tmp, default="0123456789")

    # Prefer the helper if you defined it earlier; otherwise fallback to manual build
    try:
        vocab = build_vocab_from_dataset(entries_tmp, base_tokens=base_tokens)
    except NameError:
        vocab = Vocab(base_tokens=base_tokens)
        for e in entries_tmp:
            # _extract_gt_list is provided in the JSON helpers cell we wrote
            gts = _extract_gt_list(e.get("answer"))
            for s in gts:
                vocab.add_sentence(s)

    os.makedirs(os.path.dirname(VOCAB_PATH), exist_ok=True)
    with open(VOCAB_PATH, "wb") as f:
        pickle.dump(vocab, f)

    try:
        allowed_view = sorted(vocab.allowed_set())
    except Exception:
        SPECIAL = {"<PAD>", "<SOS>", "<EOS>", "<UNK>", "<EMPTY>"}
        allowed_view = sorted([k for k in getattr(vocab, "token2idx", {}).keys() if k not in SPECIAL])

    print(f"[i] Built and saved vocab to {VOCAB_PATH}; size={len(vocab)}; allowed={allowed_view}")

# ---------- Prompt (no length hints) ----------
def build_sequence_prompt_from_bbox(valid_values: List[str], bbox: List[int]) -> str:
    vals = ''.join(valid_values)
    if bbox and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        bbox_text = f"Bounding box coordinates (x1,y1,x2,y2) = ({x1},{y1},{x2},{y2})."
    else:
        bbox_text = "No bounding box provided."
    prompt = (
        f"You are given an exam-sheet image and a rectangular region defined by coordinates. {bbox_text} "
        f"The valid symbols in this region are exactly: {vals}. "
        f"STRICTLY IGNORE any text, marks, or bubbles outside that box. "
        f"Read left-to-right, top-to-bottom within the box (row-major). "
        f"If no marked bubbles are visible, reply with the single token: EMPTY. "
        f"Return ONLY the concatenated sequence of symbols for marked bubbles with no spaces or punctuation. "
        f"Do NOT wrap your answer in backticks or markdown."
    )
    return prompt



def _strip_code_fences(text: str) -> str:
    """
    If the model put the answer in a code block (``` ... ```), prefer the LAST fenced block.
    Otherwise, return original text.
    """
    m = re.findall(r"```(?:json|text)?\\s*([\\s\\S]*?)```", text)
    if m:
        return m[-1]
    return text


def _ascii_digits_and_letters(s: str) -> str:
    """
    Normalize common fullwidth digits/letters to ASCII.
    """
    fullwidth_digits = {ord(c): ord('0') + i for i, c in enumerate("０１２３４５６７８９")}
    fullwidth_upper  = {ord(c): ord('A') + i for i, c in enumerate("ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ")}
    fullwidth_lower  = {ord(c): ord('a') + i for i, c in enumerate("ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ")}
    return s.translate({**fullwidth_digits, **fullwidth_upper, **fullwidth_lower})


def extract_sequence_from_text(
    text: str,
    vocab: Vocab,
    max_length: Optional[int] = None,          # değişken dursun ama kullanmayacağız
    allowed_values: Optional[List[str]] = None
) -> str:
    if text is None:
        return ""

    text = _strip_code_fences(str(text))
    text = _ascii_digits_and_letters(text)

    # Allowed charset
    SPECIAL = {"<PAD>", "<SOS>", "<EOS>", "<UNK>", "<EMPTY>"}
    if allowed_values:
        allowed = set(allowed_values)
    else:
        allowed = {tk for tk in getattr(vocab, "token2idx", {}).keys() if tk not in SPECIAL}


    candidates = []


    lines = [l.strip() for l in re.split(r"[\r\n]+", text) if l.strip()]
    for line in lines:
        if line.upper() == "EMPTY":
            candidates.append("")
            continue
        filt = "".join(ch for ch in line if ch in allowed)
        if filt:
            candidates.append(filt)


    pattern = "[" + "".join(re.escape(c) for c in allowed) + "]+"
    candidates += re.findall(pattern, text)

    min_run_len = 2
    candidates = [c for c in candidates if c is not None]

    if any(c == "" for c in candidates):
        return ""
    candidates = [c for c in candidates if len(c) >= min_run_len]

    if not candidates:
        return ""


    best = sorted(enumerate(candidates), key=lambda t: (len(t[1]), t[0]))[-1][1]

    return best

def infer_sequence_full(
    image_path: str,
    bbox: List[int],
    entry: dict,
    form_id: int,
    vocab: Vocab,
    debug: bool=False,
    full_img_obj: Optional[Image.Image]=None  # <-- YENİ
) -> str:
    try:
        full_img = full_img_obj if full_img_obj is not None else load_full_image(image_path)
    except Exception as e:
        if debug:
            print(f"[E] load_full_image failed for {image_path}: {e}")
        return ""

    vals = entry.get("values", list("0123456789"))
    prompt_values = list(vals) if isinstance(vals, str) else [str(v) for v in vals]

    crop = crop_in_memory(full_img, bbox, out_size=(512, 512), pad=6, apply_threshold=False)
    prompt = build_sequence_prompt_from_bbox(prompt_values, bbox)

    images = [crop] if crop is not None else [full_img]
    max_new = 64

    try:
        resp_text = nano_generate(images, prompt, max_new_tokens=max_new)
    except Exception as e:
        if debug:
            print(f"[E] nano_generate failed for {image_path}, form {form_id}: {e}")
        return ""

    seq = extract_sequence_from_text(
        resp_text,
        vocab,
        max_length=None,
        allowed_values=prompt_values
    )

    if debug:
        print(f"[dbg] resp_text[:200]: {str(resp_text)[:200]}")
        print(f"[dbg] seq -> {seq}")

    return seq

def _normalize_seq(s: str) -> str:
    if s is None:
        return ""
    s = str(s).strip()
    fullwidth_digits = {ord(c): ord('0') + i for i, c in enumerate("０１２３４５６７８９")}
    fullwidth_upper  = {ord(c): ord('A') + i for i, c in enumerate("ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ")}
    fullwidth_lower  = {ord(c): ord('a') + i for i, c in enumerate("ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ")}
    s = s.translate({**fullwidth_digits, **fullwidth_upper, **fullwidth_lower})
    return s

def exact_match(pred: str, gold: str) -> bool:
    return _normalize_seq(pred) == _normalize_seq(gold)

def char_accuracy(pred: str, gold: str) -> float:
    pred = _normalize_seq(pred)
    gold = _normalize_seq(gold)
    L = max(len(pred), len(gold))
    if L == 0:
        return 0.0
    pred = pred.ljust(L)
    gold = gold.ljust(L)
    return sum(p == g for p, g in zip(pred, gold)) / L

# Prefer editdistance; fallback to rapidfuzz; else fallback to 1 - char_accuracy
try:
    import editdistance as _ed
    def compute_cer(pred: str, gold: str) -> float:
        pred = _normalize_seq(pred)
        gold = _normalize_seq(gold)
        if len(gold) == 0:
            return float(len(pred) > 0)
        return _ed.eval(pred, gold) / len(gold)
except Exception:
    try:
        from rapidfuzz.distance import Levenshtein as _lev
        def compute_cer(pred: str, gold: str) -> float:
            pred = _normalize_seq(pred)
            gold = _normalize_seq(gold)
            if len(gold) == 0:
                return float(len(pred) > 0)
            # normalized_distance in [0,1]
            return float(_lev.normalized_distance(pred, gold))
    except Exception:
        def compute_cer(pred: str, gold: str) -> float:
            pred = _normalize_seq(pred)
            gold = _normalize_seq(gold)
            if len(gold) == 0:
                return float(len(pred) > 0)
            return 1.0 - char_accuracy(pred, gold)

def evaluate_full_images(
    entries: List[dict],
    vocab: Vocab,
    save: bool = True,
    limit: Optional[int] = None,
    resume: bool = True,
    checkpoint_every: int = 200,
) -> dict:
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, "nanonets_eval_fullimages_v2.json")

    # ---- Resume: önceki sonuçları yükle
    details: List[dict] = []
    processed_keys = set()  # (image, form_id)
    if resume and save and os.path.exists(out_path):
        try:
            with open(out_path, "r", encoding="utf-8") as f:
                old = json.load(f)
            details = old.get("details", []) or []
            for d in details:
                processed_keys.add((d.get("image"), int(d.get("form_id", 0))))
            print(f"[i] Resume: loaded {len(details)} previous results from {out_path}")
        except Exception as e:
            print(f"[W] Resume failed: {e}. Starting fresh.")

    # ---- Sayaçları eski detaylardan yeniden hesapla
    total = len(details)
    exact = sum(1 for d in details if d.get("ok"))
    char_acc_sum = sum(float(d.get("char_acc", 0.0)) for d in details)
    cer_sum = sum(float(d.get("cer", 0.0)) for d in details)
    processed = total

    # ---- Yeni örnekleri dolaş
    for ei, entry in enumerate(entries):
        img_path = entry.get('path')
        if not img_path:
            continue

        # entry başına resmi sadece 1 kez yükle
        try:
            full_img = load_full_image(img_path)
        except Exception as e:
            print(f"[E] Cannot load image {img_path}: {e}")
            continue

        areas = entry.get("answer_area", [])
        if isinstance(areas, list) and len(areas) == 4 and isinstance(areas[0], (int, float)):
            areas = [areas]
        nforms = entry.get("nforms", len(areas))

        for form_id in range(nforms):
            if limit and processed >= limit:
                break

            key = (img_path, form_id)
            if key in processed_keys:
                continue  # zaten yapılmış

            bbox = areas[form_id] if form_id < len(areas) else None
            gt   = get_gt_for_form(entry, form_id)

            pred = infer_sequence_full(
                img_path, bbox, entry, form_id, vocab,
                debug=False,
                full_img_obj=full_img  # <-- YENİ: yeniden yüklemeyi engeller
            )

            ok        = exact_match(pred, gt) if gt is not None else False
            ca        = char_accuracy(pred, gt) if gt is not None else 0.0
            cer_value = compute_cer(pred, gt or "")

            processed   += 1
            total       += 1
            exact       += int(ok)
            char_acc_sum += ca
            cer_sum     += cer_value

            details.append({
                "image": img_path,
                "form_id": form_id,
                "bbox": bbox,
                "values": entry.get("values"),
                "gt": gt,
                "pred": pred,
                "ok": ok,
                "char_acc": ca,
                "cer": cer_value
            })

            # ara kayıt + bellek temizliği
            if save and processed % checkpoint_every == 0:
                acc = (exact / total) if total else 0.0
                avg_char_acc = (char_acc_sum / total) if total else 0.0
                avg_cer = (cer_sum / total) if total else 0.0
                tmp = {
                    "model": MODEL_ID,
                    "mode": "full-page",
                    "total": total,
                    "exact_matches": exact,
                    "accuracy": acc,
                    "avg_char_accuracy": avg_char_acc,
                    "avg_cer": avg_cer,
                    "details": details
                }
                with open(out_path, "w", encoding="utf-8") as f:
                    json.dump(tmp, f, indent=2, ensure_ascii=False)
                print(f"[i] checkpoint @ {processed} -> saved to {out_path}")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()

        if limit and processed >= limit:
            break

    acc = (exact / total) if total else 0.0
    avg_char_acc = (char_acc_sum / total) if total else 0.0
    avg_cer = (cer_sum / total) if total else 0.0

    out = {
        "model": MODEL_ID,
        "mode": "full-page",
        "total": total,
        "exact_matches": exact,
        "accuracy": acc,
        "avg_char_accuracy": avg_char_acc,
        "avg_cer": avg_cer,
        "details": details
    }

    if save:
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(out, f, indent=2, ensure_ascii=False)
        print(f"[i] Saved evaluation to {out_path}")

    return out

# Ensure get_gt_for_form exists (in case JSON helpers cell wasn't executed in this session)
from typing import Any, List, Optional

def _extract_gt_list(ans_field: Any) -> List[str]:
    out = []
    if ans_field is None:
        return out
    if isinstance(ans_field, str):
        out.append(ans_field); return out
    if isinstance(ans_field, list):
        for item in ans_field:
            if isinstance(item, str):
                out.append(item)
            elif isinstance(item, list) and item and isinstance(item[0], str):
                out.append(item[0])
    return out

try:
    get_gt_for_form
except NameError:
    def get_gt_for_form(entry: dict, form_id: int) -> Optional[str]:
        if '_gt_list' in entry and isinstance(entry['_gt_list'], list):
            if 0 <= form_id < len(entry['_gt_list']):
                return entry['_gt_list'][form_id] or None
            if entry['_gt_list']:
                return entry['_gt_list'][0] or None

        answers = entry.get('answer', [])
        if isinstance(answers, list) and form_id < len(answers) and answers[form_id]:
            if isinstance(answers[form_id], str):
                return answers[form_id]
            if isinstance(answers[form_id], list) and answers[form_id] and isinstance(answers[form_id][0], str):
                return answers[form_id][0]
        if isinstance(answers, list) and answers:
            if isinstance(answers[0], str):
                return answers[0]
            if isinstance(answers[0], list) and answers[0] and isinstance(answers[0][0], str):
                return answers[0][0]
        if isinstance(answers, str):
            return answers
        return None

print("[i] get_gt_for_form is ready:", "OK")

# Put this near MAIN (before using test_images loop)
def find_entry_by_path(entries, p):
    for e in entries:
        if e.get("path") == p:
            return e
    return None

# ---------- MAIN ----------
if __name__ == "__main__":
    gc.collect()

    # Yükle
    entries = load_json_entries(JSON_PATH)
    lookup  = build_lookup_map(entries)

    # Tüm dataset değerlendirme (resume + checkpoint açık)
    print("[i] Running FULL DATASET evaluation")
    results = evaluate_full_images(
        entries,
        vocab,
        save=True,
        limit=None,          # <-- TÜM SET
        resume=True,         # <-- kaldığı yerden devam eder
        checkpoint_every=200 # her 200 formda ara kayıt
    )

    print("[i] Eval summary:", {
        'total': results['total'],
        'exact_matches': results['exact_matches'],
        'accuracy': results['accuracy'],
        'avg_char_accuracy': results['avg_char_accuracy'],
        'avg_cer': results['avg_cer']
    })
    print("[i] Done.")